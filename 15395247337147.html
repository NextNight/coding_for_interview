<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
  
  词向量 - Kaka
  
  </title>
 <meta name="description" content="面试，刷题，扯淡。。">
 <link href="atom.xml" rel="alternate" title="Kaka" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />

    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
    <script src="asset/highlightjs/highlight.pack.js"></script>
    <script src="asset/highlightjs/mermaid.min.js"></script>
    <link href="asset/highlightjs/styles/github-gist.css" media="screen, projection" rel="stylesheet" type="text/css">
    <script>hljs.initHighlightingOnLoad();</script>
    <!--add  -->
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/7.1.0/mermaid.min.js"></script>
    <script>
        var config = {
            startOnLoad:true,
            flowchart:{
                useMaxWidth:false,
                htmlLabels:true
            }
        };
        mermaid.initialize(config);
        $(function(){
            var elements = document.getElementsByClassName("language-mermaid");
            for (var i = elements.length; i--;) {
                element = elements[i];
                var graphDefinition = element.innerText;
                if (graphDefinition) {
                    var svg = mermaid.render('ha_mermaid_' + i, graphDefinition, function(svg){});
                    if (svg) {
                        var svgElement = document.createElement('div');
                        preNode = element.parentNode;
                        svgElement.innerHTML = svg;
                        svgElement.setAttribute('class', 'mermaid');
                        svgElement.setAttribute('data-processed', 'true');
                        preNode.parentNode.replaceChild(svgElement, preNode);
                    }
                }
            }
        });
    </script>
  </head>
  <body class="antialiased hide-extras">

    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>

<div id="header">
    <h1><a href="index.html">Kaka</a></h1>
</div>

</nav>
        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; Kaka</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
      <li><a href="index.html">Home</a></li>
      
        <li class="divider"></li>
        <li><label>数据结构</label></li>

          
            <li><a title="数据结构" href="15510643590406.html">数据结构</a></li>
          
            <li><a title="数组：Array" href="15391804069666.html">数组：Array</a></li>
          
            <li><a title="链表：LinkedList" href="15391807987921.html">链表：LinkedList</a></li>
          
            <li><a title="栈：Stack" href="15392291360952.html">栈：Stack</a></li>
          
            <li><a title="队列：Queue" href="15391808453198.html">队列：Queue</a></li>
          
            <li><a title="堆：Heap" href="15392363328863.html">堆：Heap</a></li>
          
            <li><a title="图：Graph" href="15392292553953.html">图：Graph</a></li>
          
            <li><a title="哈希表：Hash" href="15392292427453.html">哈希表：Hash</a></li>
          
            <li><a title="树：Tree" href="15392291769523.html">树：Tree</a></li>
          

      
        <li class="divider"></li>
        <li><label>算法设计</label></li>

          
            <li><a title="🐱 查找问题" href="15393054098142.html">🐱 查找问题</a></li>
          
            <li><a title="二分查找" href="15395253854404.html">二分查找</a></li>
          
            <li><a title="🐱 排序问题" href="15392700147620.html">🐱 排序问题</a></li>
          
            <li><a title="计数排序" href="15392716754828.html">计数排序</a></li>
          
            <li><a title="希尔排序" href="15392716677172.html">希尔排序</a></li>
          
            <li><a title="桶排序" href="15392715483846.html">桶排序</a></li>
          
            <li><a title="归并排序" href="15392715143262.html">归并排序</a></li>
          
            <li><a title="快速排序" href="15392714966006.html">快速排序</a></li>
          
            <li><a title="冒泡排序" href="15392714719077.html">冒泡排序</a></li>
          
            <li><a title="插入排序" href="15392697680749.html">插入排序</a></li>
          
            <li><a title="选择排序" href="15391804641073.html">选择排序</a></li>
          
            <li><a title="Markdown语法" href="15391839924703.html">Markdown语法</a></li>
          

      
        <li class="divider"></li>
        <li><label>算法思想</label></li>

          
            <li><a title="时间复杂度与空间复杂度" href="15500647845613.html">时间复杂度与空间复杂度</a></li>
          
            <li><a title="分治" href="15393053301597.html">分治</a></li>
          
            <li><a title="回溯" href="15393053101796.html">回溯</a></li>
          
            <li><a title="动态规划" href="15392704829367.html">动态规划</a></li>
          
            <li><a title="贪心" href="15392704711231.html">贪心</a></li>
          
            <li><a title="递归" href="15392704564412.html">递归</a></li>
          

      
        <li class="divider"></li>
        <li><label>Leetcode</label></li>

          
            <li><a title="Twonumsum" href="15392386512557.html">Twonumsum</a></li>
          

      
        <li class="divider"></li>
        <li><label>深度学习</label></li>

          
            <li><a title="深度学习库keras" href="15421840556162.html">深度学习库keras</a></li>
          
            <li><a title="Keras中的数据预处理「文本」" href="15429415402935.html">Keras中的数据预处理「文本」</a></li>
          
            <li><a title="文本分类TextCNN" href="15434622232762.html">文本分类TextCNN</a></li>
          
            <li><a title="词向量" href="15395247337147.html">词向量</a></li>
          
            <li><a title="文本分类模型" href="15395241741886.html">文本分类模型</a></li>
          

      
        <li class="divider"></li>
        <li><label>机器学习</label></li>

          
            <li><a title="HMM到中文分词" href="15446854936789.html">HMM到中文分词</a></li>
          

      
        <li class="divider"></li>
        <li><label>论文相关</label></li>

          
            <li><a title="论文collections" href="15488331878996.html">论文collections</a></li>
          
            <li><a title="文本分类的注意力模型" href="15395117974496.html">文本分类的注意力模型</a></li>
          

      
        <li class="divider"></li>
        <li><label>公式推导</label></li>

          
            <li><a title="逻辑回归" href="15392705055223.html">逻辑回归</a></li>
          

      
        <li class="divider"></li>
        <li><label>开始瞎扯淡</label></li>

          
            <li><a title="吴翼：我的ACM参赛故事" href="15391808991416.html">吴翼：我的ACM参赛故事</a></li>
          
            <li><a title="" href="15392402403847.html"></a></li>
          
            <li><a title="我这一生" href="15391740524915.html">我这一生</a></li>
          

      
      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>

        <section id="main-content" role="main" class="scroll-container">

          <div class="row">
            <div class="large-3 medium-3 columns">
              <div class="hide-for-small">
                <div class="sidebar">
                <nav>
                  <ul id="side-nav" class="side-nav">

                    
                      <li class="side-title"><span>数据结构</span></li>
                        
                          <li><a title="数据结构" href="15510643590406.html">数据结构</a></li>
                        
                          <li><a title="数组：Array" href="15391804069666.html">数组：Array</a></li>
                        
                          <li><a title="链表：LinkedList" href="15391807987921.html">链表：LinkedList</a></li>
                        
                          <li><a title="栈：Stack" href="15392291360952.html">栈：Stack</a></li>
                        
                          <li><a title="队列：Queue" href="15391808453198.html">队列：Queue</a></li>
                        
                          <li><a title="堆：Heap" href="15392363328863.html">堆：Heap</a></li>
                        
                          <li><a title="图：Graph" href="15392292553953.html">图：Graph</a></li>
                        
                          <li><a title="哈希表：Hash" href="15392292427453.html">哈希表：Hash</a></li>
                        
                          <li><a title="树：Tree" href="15392291769523.html">树：Tree</a></li>
                        

                    
                      <li class="side-title"><span>算法设计</span></li>
                        
                          <li><a title="🐱 查找问题" href="15393054098142.html">🐱 查找问题</a></li>
                        
                          <li><a title="二分查找" href="15395253854404.html">二分查找</a></li>
                        
                          <li><a title="🐱 排序问题" href="15392700147620.html">🐱 排序问题</a></li>
                        
                          <li><a title="计数排序" href="15392716754828.html">计数排序</a></li>
                        
                          <li><a title="希尔排序" href="15392716677172.html">希尔排序</a></li>
                        
                          <li><a title="桶排序" href="15392715483846.html">桶排序</a></li>
                        
                          <li><a title="归并排序" href="15392715143262.html">归并排序</a></li>
                        
                          <li><a title="快速排序" href="15392714966006.html">快速排序</a></li>
                        
                          <li><a title="冒泡排序" href="15392714719077.html">冒泡排序</a></li>
                        
                          <li><a title="插入排序" href="15392697680749.html">插入排序</a></li>
                        
                          <li><a title="选择排序" href="15391804641073.html">选择排序</a></li>
                        
                          <li><a title="Markdown语法" href="15391839924703.html">Markdown语法</a></li>
                        

                    
                      <li class="side-title"><span>算法思想</span></li>
                        
                          <li><a title="时间复杂度与空间复杂度" href="15500647845613.html">时间复杂度与空间复杂度</a></li>
                        
                          <li><a title="分治" href="15393053301597.html">分治</a></li>
                        
                          <li><a title="回溯" href="15393053101796.html">回溯</a></li>
                        
                          <li><a title="动态规划" href="15392704829367.html">动态规划</a></li>
                        
                          <li><a title="贪心" href="15392704711231.html">贪心</a></li>
                        
                          <li><a title="递归" href="15392704564412.html">递归</a></li>
                        

                    
                      <li class="side-title"><span>Leetcode</span></li>
                        
                          <li><a title="Twonumsum" href="15392386512557.html">Twonumsum</a></li>
                        

                    
                      <li class="side-title"><span>深度学习</span></li>
                        
                          <li><a title="深度学习库keras" href="15421840556162.html">深度学习库keras</a></li>
                        
                          <li><a title="Keras中的数据预处理「文本」" href="15429415402935.html">Keras中的数据预处理「文本」</a></li>
                        
                          <li><a title="文本分类TextCNN" href="15434622232762.html">文本分类TextCNN</a></li>
                        
                          <li><a title="词向量" href="15395247337147.html">词向量</a></li>
                        
                          <li><a title="文本分类模型" href="15395241741886.html">文本分类模型</a></li>
                        

                    
                      <li class="side-title"><span>机器学习</span></li>
                        
                          <li><a title="HMM到中文分词" href="15446854936789.html">HMM到中文分词</a></li>
                        

                    
                      <li class="side-title"><span>论文相关</span></li>
                        
                          <li><a title="论文collections" href="15488331878996.html">论文collections</a></li>
                        
                          <li><a title="文本分类的注意力模型" href="15395117974496.html">文本分类的注意力模型</a></li>
                        

                    
                      <li class="side-title"><span>公式推导</span></li>
                        
                          <li><a title="逻辑回归" href="15392705055223.html">逻辑回归</a></li>
                        

                    
                      <li class="side-title"><span>开始瞎扯淡</span></li>
                        
                          <li><a title="吴翼：我的ACM参赛故事" href="15391808991416.html">吴翼：我的ACM参赛故事</a></li>
                        
                          <li><a title="" href="15392402403847.html"></a></li>
                        
                          <li><a title="我这一生" href="15391740524915.html">我这一生</a></li>
                        

                    
                  </ul>
                </nav>
                </div>
              </div>
            </div>
            <div class="large-9 medium-9 columns">
 <div class="markdown-body">
<h1>词向量</h1>

<p>在所有的自然语言任务中向量化是必不可少的一步，也是所有任务的第一步，只有将数据编码成数值向量才能进行后续的任务，一个好的向量化表示方法能够表示更多的内涵意义，从而让词包含更多的信息。</p>

<h1 id="toc_0">如何去表示一个字/单词</h1>

<p>在所有NLP任务中，第一个也是最重要的公共问题是我们如何将单词表示为任何模型的输入。早期NLP的许多工作都将单词视为原子符号。为了在大多数NLP任务中表现出色，我们首先需要对单词之间的相似性和差异有一些概念。有了字向量，我们可以很容易解决这些问题(使用距离度量，如jaccard、cosine、eu-clidean等)不管是英文，还是中文都存在大量的单词词汇，这些词汇中间大所存在着或多或少的关系，我们希望可以将这些词映射到某个<code>词空间中</code>作为这个空间中某个点。这样这个点在空间中的向量化表示就可以用来表示这个词了，同时可以在这个空间中可以度量词之间的关系。而最简单的词空间就是整个词汇表空间。</p>

<h1 id="toc_1">One-Hot</h1>

<p>One-Hot是一种非常简单的词向量表示，它使用\(R^{|V|*1}\)大小的向量表示每个单词，其中\(|V|\)表示词汇表大的小，如要表示‘A’:它以词汇表大小n构建一个长度为n的向量[0,0,0,....1...0]，向量中除了单词‘A’所在的位置为1，其他位置都为0.<br/>
<img src="media/15395247337147/15529937344598.jpg" alt="" style="width:489px;"/></p>

<p>但是我们发现这种表示方法把每个单词表示成单独的个体，无法体现出他们之间的联系，而且数据稀疏，空间巨大。因此我们可以将它映射到更小的空间里.</p>

<h1 id="toc_2">基于SVD的向量表示</h1>

<p>SVD:奇异值分解，是一种矩阵分解方法，遍历数据集得到一个共现矩阵，然后进行奇异值分解，选择前K个奇异值，对原始矩阵进行降维，就可以得到一个低维的词向量表示。以下是基于两种共现矩阵的词向量表示。</p>

<h3 id="toc_3">单词-文档共现矩阵</h3>

<p>词-文档的共现矩阵能够表示出<code>相关</code>的词，那么如何用这个矩阵来表示词向量呢，就是以文档的大小作为单词向量的大小，在进行SVD分解得到映射后的向量。假如有M篇文档，有V个词汇，那么这个共现矩阵大小为M*N,而对于新增文档，新增词，这个矩阵就必须更新。</p>

<h3 id="toc_4">单词-单词共现矩阵</h3>

<p>单词-单词的矩阵同样能够表示单词之间的<code>相关性</code>.而单词的量级变化没有文档数变化那么大，相比于单词-文档矩阵，这个矩阵要小很多。采用同样的矩阵分解方式可以得到新的词向量表示。</p>

<p>降维前：<br/>
<img src="media/15395247337147/15530533751931.jpg" alt="" style="width:595px;"/><br/>
降维后：<br/>
<img src="media/15395247337147/15530533964758.jpg" alt="" style="width:565px;"/></p>

<h3 id="toc_5">优势与存在的问题</h3>

<h4 id="toc_6">优势：</h4>

<ul>
<li>有效利用了统计的信息</li>
<li>矩阵分解可以得到部分语义信息</li>
</ul>

<h4 id="toc_7">缺点：</h4>

<ul>
<li>维护的矩阵太大，且非常稀疏(非常多数数据没有共现)</li>
<li>新增数据或单词都要对矩阵进行更新</li>
<li>需要频繁的进行矩阵分解</li>
</ul>

<h3 id="toc_8">解决方案</h3>

<ul>
<li>忽略无用词「无意义」 </li>
<li>使用窗口，即使用文档中单词的距离进行共现计数加权</li>
</ul>

<h1 id="toc_9">基于迭代的词向量-word2vec</h1>

<blockquote>
<p>放弃计算和存储大型的数据矩阵，神经网络兴起之后，我们尝试通过训练一个model来得到单词的词向量，而word2vec的思想就是训练一个参数为词向量的模型,这个模型我们所说的<code>语言模型</code>.当我们得到一个优化的语言模型的时候就得到了一个高质量的词向量(model的隐层参数)。</p>
</blockquote>

<h2 id="toc_10">语言模型（Langue Model）</h2>

<p>语言模型描述的是一个句子(单词序列)的合理性，一个好的语言模型能够给一个合理的句子更高的分数(概率)。加入一个句子s由n个字组成那么他的概率可以表示如下：<br/>
\[<br/>
P_s = P(w_1,w_2...w_n) = \prod _{ i=1 }^{ n }{ P(w_i) } \tag{Unigrams:基于NB条件独立假设}<br/>
\]<br/>
以上是基于条件独立假设的Unigrams模型：假设句子中的每个单词/字相互独立，那么这个句子的概率值就是每个单词/字的概率积，很显然这种假设太过简单，以至于无论几个单词如何组合他们的概率都是相同的。而一句话中每个单词/字的概率是依赖于上下文的，那么句子s的概率可以表示如下：<br/>
\[<br/>
P_s = P(w_1,w_2...w_n) = \prod _{ i=2 }^{ n }{ P(w_i|w_{i-1}) } \tag{Bigrams:基于马尔科夫模型}<br/>
\]<br/>
以上是基于马尔科夫模型的Bigrams:假设当前词的概率依赖于前一个词的概率，那这个句子的概率就是单词、字的条件概率积。这种模型相比于前面的Unigrams提供更多上下文和语序信息。</p>

<blockquote>
<p>以上就是我们所说的<code>N-gram</code>语言模型，N表示当前词的概率依赖于前N-1个词。在这里我们知道了如何去计算了一个句子的概率，那么我们需要如何去训练整改率呢??「『即如何使得这个概率最大化』」</p>
</blockquote>

<h2 id="toc_11">word2vec两种训练算法：</h2>

<ul>
<li><p>cbow:上下文词袋模型，以上下文词预测这个中心词w。</p></li>
<li><p>skip-gram:以一个中心词W去预测上下文词。</p></li>
</ul>

<p>wordvec的语言模型非常简单，是一个三层的神经网络，包含输入层-隐藏层-输出层如下图：右侧：CBOW,左侧：skip-gram<br/>
<img src="media/15395247337147/15530608575223.jpg" alt="" style="width:282px;"/><img src="media/15395247337147/15530622531387.jpg" alt="" style="width:259px;"/> </p>

<p>参数解析：<hr></p>

<ul>
<li>\(W\):单词</li>
<li>\(V\):词汇表大小</li>
<li>\(N\):我们要得到的词向量的大小</li>
<li>\(k\):第k个中心词</li>
</ul>

<h3 id="toc_12">CBOW模型的训练步骤：</h3>

<ul>
<li>1. 输入的上下文词的one-hot表示。</li>
<li>2. 根据\(W_{V*N}\)输入词矩阵得到上下文词的词嵌入(embding)表示。</li>
<li>3. 将得到的上下文词的词嵌入表示求平均值，得到一个N维的向量。</li>
<li>4. 将这个向量与中心词矩阵\(W&#39;_{N*V}\)相乘，就得到了一个V维的向量，他的每一个值表示的上下文预测的中心词的可能性大小。</li>
<li>5. 为了更好的描述这种可能性大小，我们将其接入softmax，进行归一化城概率值。</li>
<li>6. 将这个概率向量与真正的中心词向量比较就可以得到误差从而更新我们要训练的\(W，W&#39;\)矩阵。</li>
</ul>

<blockquote>
<p>⚠️步骤2中提到得到词嵌入表示，那么这个词嵌入是如何得到的呢？，事实上\(W_{V*N}\)矩阵就是一个<code>词向量矩阵</code>「这个矩阵是由一个均匀分布随机生成的」，而one-hot向量不过是从中选择了属于自己的那个而已。所以：我们在构建一个预训练词嵌入的模型的时候，既可以把词向量作为一个权重参数，也可以直接用词向量去替换成文档的向量表示。</p>
</blockquote>

<p>知道如何训练，那么如何对训练的结果用于优化模型参数呢？需要定义目标函数和损失函数，采用SGD迭代求解的方式不断地去更新参数。</p>

<h2 id="toc_13">损失函数与目标函数</h2>

<p>目标函数:最大化上下文预测的中心词的概率。<br/>
\[\begin{aligned}<br/>
skip-gram-obj&amp;= \prod_{j=0}^{2m} P(W_{c-m+j...c-m+j}|W_i)\\<br/>
context-bow-obj&amp;= P(W_i|W_{c-m+j...c-m+j}) <br/>
\end{aligned}<br/>
\]<br/>
m表示上下文窗口大小m=2就是上下文各选2个词</p>

<p>损失函数：交叉熵<br/>
\[\begin{aligned}<br/>
skip-gram H(y,y&#39;)&amp;= -ylog(y&#39;)\\<br/>
&amp;=-1*log(y&#39;) \\<br/>
&amp;=-log{P(W_{c-m...c-m}|W_c)} \\<br/>
&amp;=-log{\prod_{j=0,j\neq m}^{2m}P(W_{c-m+j}|W_c)} \\<br/>
&amp;=-log{\prod_{j=0,j\neq m}^{2m}P(u_{c-m+j}|v&#39;)}\\<br/>
&amp;=-log\prod_{j=0,j\neq m}^{2m}(\frac {exp^{u_{c-m+j}^Tv&#39;}}{\sum_{k=0}^{V}exp^{u_k^Tv&#39;}})\\<br/>
J&amp;=-\sum_{j=0,j\neq m}^{2m} u_{c-m+j}^Tv&#39;+log \sum_{i=0}^{V}exp^{u_i^Tv&#39;}\\<br/>
\end{aligned}<br/>
\]<br/>
\[\begin{aligned}<br/>
context-bow H(y,y&#39;)&amp;= -ylog(y&#39;)\\<br/>
&amp;=-1*log(y&#39;) \\<br/>
&amp;=-log{P(W_c|W_{c-m...c+m})} \\<br/>
&amp;=-log{P(u_c|v&#39;)}\\<br/>
&amp;=-log{\frac {exp^{u_c^Tv&#39;}}{\sum_{k=0}^{V}exp^{u_k^Tv&#39;}}}\\<br/>
J&amp;=-u_c^Tv&#39;+log{\sum_{i=0}^{V}{u_i^Tv&#39;}}\\<br/>
\end{aligned}<br/>
\]<br/>
有了损失函数，就可以求解梯度\(▽_{J(\theta)}\)，从而更新我们的参数\(u_c=u_c-α▽_J\)</p>

<h2 id="toc_14">word2vec两种优化算法：</h2>

<ul>
<li>hierarchical softmax：层次化softmax</li>
<li>Negative sampling：负采样</li>
</ul>


</div>

<br /><br />
<hr />

<div class="row clearfix">
  <div class="large-6 columns">
	<div class="text-left" style="padding:15px 0px;">
		
	        <a href="15434622232762.html"  title="Previous Post: 文本分类TextCNN">&laquo; 文本分类TextCNN</a>
	    
	</div>
  </div>
  <div class="large-6 columns">
	<div class="text-right" style="padding:15px 0px;">
		
	        <a href="15395241741886.html" 
	        title="Next Post: 文本分类模型">文本分类模型 &raquo;</a>
	    
	</div>
  </div>
</div>

<div class="row">
<div style="padding:0px 0.93em;" class="share-comments">

</div>
</div>
<script type="text/javascript">
	$(function(){
		var currentURL = '15395247337147.html';
		$('#side-nav a').each(function(){
			if($(this).attr('href') == currentURL){
				$(this).parent().addClass('active');
			}
		});
	});
</script>  
</div></div>


<div class="page-bottom">
  <div class="row">
  <hr />
  <div class="small-9 columns">
  <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
  <div class="small-3 columns">
  <p class="copyright text-right"><a href="#header">TOP</a></p>
  </div>
   
  </div>
</div>

        </section>
      </div>
    </div>
    
    
    <script src="asset/js/foundation.min.js"></script>
    <script src="asset/js/foundation/foundation.offcanvas.js"></script>
    <script>
      $(document).foundation();

     
    </script>
    
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>

  </body>
</html>
