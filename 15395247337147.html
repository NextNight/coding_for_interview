<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
  
  词向量发展历程<一> - Kaka
  
  </title>
 <meta name="description" content="面试，刷题，扯淡。。">
 <link href="atom.xml" rel="alternate" title="Kaka" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />

    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
    <script src="asset/highlightjs/highlight.pack.js"></script>
    <script src="asset/highlightjs/mermaid.min.js"></script>
    <link href="asset/highlightjs/styles/github-gist.css" media="screen, projection" rel="stylesheet" type="text/css">
    <script>hljs.initHighlightingOnLoad();</script>
    <!--add  -->
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/7.1.0/mermaid.min.js"></script>
    <script>
        var config = {
            startOnLoad:true,
            flowchart:{
                useMaxWidth:false,
                htmlLabels:true
            }
        };
        mermaid.initialize(config);
        $(function(){
            var elements = document.getElementsByClassName("language-mermaid");
            for (var i = elements.length; i--;) {
                element = elements[i];
                var graphDefinition = element.innerText;
                if (graphDefinition) {
                    var svg = mermaid.render('ha_mermaid_' + i, graphDefinition, function(svg){});
                    if (svg) {
                        var svgElement = document.createElement('div');
                        preNode = element.parentNode;
                        svgElement.innerHTML = svg;
                        svgElement.setAttribute('class', 'mermaid');
                        svgElement.setAttribute('data-processed', 'true');
                        preNode.parentNode.replaceChild(svgElement, preNode);
                    }
                }
            }
        });
    </script>
  </head>
  <body class="antialiased hide-extras">

    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>

<div id="header">
    <h1><a href="index.html">Kaka</a></h1>
</div>

</nav>
        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; Kaka</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
      <li><a href="index.html">Home</a></li>
      
        <li class="divider"></li>
        <li><label>数据结构</label></li>

          
            <li><a title="数据结构" href="15510643590406.html">数据结构</a></li>
          
            <li><a title="数组：Array" href="15391804069666.html">数组：Array</a></li>
          
            <li><a title="链表：LinkedList" href="15391807987921.html">链表：LinkedList</a></li>
          
            <li><a title="栈：Stack" href="15392291360952.html">栈：Stack</a></li>
          
            <li><a title="队列：Queue" href="15391808453198.html">队列：Queue</a></li>
          
            <li><a title="堆：Heap" href="15392363328863.html">堆：Heap</a></li>
          
            <li><a title="图：Graph" href="15392292553953.html">图：Graph</a></li>
          
            <li><a title="哈希表：Hash" href="15392292427453.html">哈希表：Hash</a></li>
          
            <li><a title="树：Tree" href="15392291769523.html">树：Tree</a></li>
          

      
        <li class="divider"></li>
        <li><label>算法设计</label></li>

          
            <li><a title="🐱 查找问题" href="15393054098142.html">🐱 查找问题</a></li>
          
            <li><a title="二分查找" href="15395253854404.html">二分查找</a></li>
          
            <li><a title="🐱 排序问题" href="15392700147620.html">🐱 排序问题</a></li>
          
            <li><a title="计数排序" href="15392716754828.html">计数排序</a></li>
          
            <li><a title="希尔排序" href="15392716677172.html">希尔排序</a></li>
          
            <li><a title="桶排序" href="15392715483846.html">桶排序</a></li>
          
            <li><a title="归并排序" href="15392715143262.html">归并排序</a></li>
          
            <li><a title="快速排序" href="15392714966006.html">快速排序</a></li>
          
            <li><a title="冒泡排序" href="15392714719077.html">冒泡排序</a></li>
          
            <li><a title="插入排序" href="15392697680749.html">插入排序</a></li>
          
            <li><a title="选择排序" href="15391804641073.html">选择排序</a></li>
          
            <li><a title="Markdown语法" href="15391839924703.html">Markdown语法</a></li>
          

      
        <li class="divider"></li>
        <li><label>算法思想</label></li>

          
            <li><a title="时间复杂度与空间复杂度" href="15500647845613.html">时间复杂度与空间复杂度</a></li>
          
            <li><a title="分治" href="15393053301597.html">分治</a></li>
          
            <li><a title="回溯" href="15393053101796.html">回溯</a></li>
          
            <li><a title="动态规划" href="15392704829367.html">动态规划</a></li>
          
            <li><a title="贪心" href="15392704711231.html">贪心</a></li>
          
            <li><a title="递归" href="15392704564412.html">递归</a></li>
          

      
        <li class="divider"></li>
        <li><label>Leetcode</label></li>

          
            <li><a title="Twonumsum" href="15392386512557.html">Twonumsum</a></li>
          

      
        <li class="divider"></li>
        <li><label>深度学习</label></li>

          
            <li><a title="深度学习中的防止过拟合机制" href="15577405434359.html">深度学习中的防止过拟合机制</a></li>
          
            <li><a title="深度学习中的优化器" href="15577404931077.html">深度学习中的优化器</a></li>
          
            <li><a title="深度学习中的激活函数" href="15577404400424.html">深度学习中的激活函数</a></li>
          
            <li><a title="深度学习中的注意力机制(Attention Mechanism)" href="15574848959443.html">深度学习中的注意力机制(Attention Mechanism)</a></li>
          
            <li><a title="深度学习库keras" href="15421840556162.html">深度学习库keras</a></li>
          
            <li><a title="数据预处理「文本」" href="15429415402935.html">数据预处理「文本」</a></li>
          
            <li><a title="文本分类TextCNN" href="15434622232762.html">文本分类TextCNN</a></li>
          
            <li><a title="词向量发展历程<一>" href="15395247337147.html">词向量发展历程<一></a></li>
          
            <li><a title="词向量发展历程<二>" href="15560941732047.html">词向量发展历程<二></a></li>
          

      
        <li class="divider"></li>
        <li><label>机器学习</label></li>

          
            <li><a title="HMM到中文分词" href="15446854936789.html">HMM到中文分词</a></li>
          

      
        <li class="divider"></li>
        <li><label>论文相关</label></li>

          
            <li><a title="论文collections" href="15488331878996.html">论文collections</a></li>
          
            <li><a title="文本分类的注意力模型" href="15395117974496.html">文本分类的注意力模型</a></li>
          

      
        <li class="divider"></li>
        <li><label>公式推导</label></li>

          
            <li><a title="逻辑回归" href="15392705055223.html">逻辑回归</a></li>
          

      
        <li class="divider"></li>
        <li><label>开始瞎扯淡</label></li>

          
            <li><a title="吴翼：我的ACM参赛故事" href="15391808991416.html">吴翼：我的ACM参赛故事</a></li>
          
            <li><a title="我这一生" href="15391740524915.html">我这一生</a></li>
          

      
      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>

        <section id="main-content" role="main" class="scroll-container">

          <div class="row">
            <div class="large-3 medium-3 columns">
              <div class="hide-for-small">
                <div class="sidebar">
                <nav>
                  <ul id="side-nav" class="side-nav">

                    
                      <li class="side-title"><span>数据结构</span></li>
                        
                          <li><a title="数据结构" href="15510643590406.html">数据结构</a></li>
                        
                          <li><a title="数组：Array" href="15391804069666.html">数组：Array</a></li>
                        
                          <li><a title="链表：LinkedList" href="15391807987921.html">链表：LinkedList</a></li>
                        
                          <li><a title="栈：Stack" href="15392291360952.html">栈：Stack</a></li>
                        
                          <li><a title="队列：Queue" href="15391808453198.html">队列：Queue</a></li>
                        
                          <li><a title="堆：Heap" href="15392363328863.html">堆：Heap</a></li>
                        
                          <li><a title="图：Graph" href="15392292553953.html">图：Graph</a></li>
                        
                          <li><a title="哈希表：Hash" href="15392292427453.html">哈希表：Hash</a></li>
                        
                          <li><a title="树：Tree" href="15392291769523.html">树：Tree</a></li>
                        

                    
                      <li class="side-title"><span>算法设计</span></li>
                        
                          <li><a title="🐱 查找问题" href="15393054098142.html">🐱 查找问题</a></li>
                        
                          <li><a title="二分查找" href="15395253854404.html">二分查找</a></li>
                        
                          <li><a title="🐱 排序问题" href="15392700147620.html">🐱 排序问题</a></li>
                        
                          <li><a title="计数排序" href="15392716754828.html">计数排序</a></li>
                        
                          <li><a title="希尔排序" href="15392716677172.html">希尔排序</a></li>
                        
                          <li><a title="桶排序" href="15392715483846.html">桶排序</a></li>
                        
                          <li><a title="归并排序" href="15392715143262.html">归并排序</a></li>
                        
                          <li><a title="快速排序" href="15392714966006.html">快速排序</a></li>
                        
                          <li><a title="冒泡排序" href="15392714719077.html">冒泡排序</a></li>
                        
                          <li><a title="插入排序" href="15392697680749.html">插入排序</a></li>
                        
                          <li><a title="选择排序" href="15391804641073.html">选择排序</a></li>
                        
                          <li><a title="Markdown语法" href="15391839924703.html">Markdown语法</a></li>
                        

                    
                      <li class="side-title"><span>算法思想</span></li>
                        
                          <li><a title="时间复杂度与空间复杂度" href="15500647845613.html">时间复杂度与空间复杂度</a></li>
                        
                          <li><a title="分治" href="15393053301597.html">分治</a></li>
                        
                          <li><a title="回溯" href="15393053101796.html">回溯</a></li>
                        
                          <li><a title="动态规划" href="15392704829367.html">动态规划</a></li>
                        
                          <li><a title="贪心" href="15392704711231.html">贪心</a></li>
                        
                          <li><a title="递归" href="15392704564412.html">递归</a></li>
                        

                    
                      <li class="side-title"><span>Leetcode</span></li>
                        
                          <li><a title="Twonumsum" href="15392386512557.html">Twonumsum</a></li>
                        

                    
                      <li class="side-title"><span>深度学习</span></li>
                        
                          <li><a title="深度学习中的防止过拟合机制" href="15577405434359.html">深度学习中的防止过拟合机制</a></li>
                        
                          <li><a title="深度学习中的优化器" href="15577404931077.html">深度学习中的优化器</a></li>
                        
                          <li><a title="深度学习中的激活函数" href="15577404400424.html">深度学习中的激活函数</a></li>
                        
                          <li><a title="深度学习中的注意力机制(Attention Mechanism)" href="15574848959443.html">深度学习中的注意力机制(Attention Mechanism)</a></li>
                        
                          <li><a title="深度学习库keras" href="15421840556162.html">深度学习库keras</a></li>
                        
                          <li><a title="数据预处理「文本」" href="15429415402935.html">数据预处理「文本」</a></li>
                        
                          <li><a title="文本分类TextCNN" href="15434622232762.html">文本分类TextCNN</a></li>
                        
                          <li><a title="词向量发展历程<一>" href="15395247337147.html">词向量发展历程<一></a></li>
                        
                          <li><a title="词向量发展历程<二>" href="15560941732047.html">词向量发展历程<二></a></li>
                        

                    
                      <li class="side-title"><span>机器学习</span></li>
                        
                          <li><a title="HMM到中文分词" href="15446854936789.html">HMM到中文分词</a></li>
                        

                    
                      <li class="side-title"><span>论文相关</span></li>
                        
                          <li><a title="论文collections" href="15488331878996.html">论文collections</a></li>
                        
                          <li><a title="文本分类的注意力模型" href="15395117974496.html">文本分类的注意力模型</a></li>
                        

                    
                      <li class="side-title"><span>公式推导</span></li>
                        
                          <li><a title="逻辑回归" href="15392705055223.html">逻辑回归</a></li>
                        

                    
                      <li class="side-title"><span>开始瞎扯淡</span></li>
                        
                          <li><a title="吴翼：我的ACM参赛故事" href="15391808991416.html">吴翼：我的ACM参赛故事</a></li>
                        
                          <li><a title="我这一生" href="15391740524915.html">我这一生</a></li>
                        

                    
                  </ul>
                </nav>
                </div>
              </div>
            </div>
            <div class="large-9 medium-9 columns">
 <div class="markdown-body">
<h1>词向量发展历程<一></h1>

<p>在所有的自然语言任务中向量化是必不可少的一步，也是所有任务的第一步，只有将数据编码成数值向量才能进行后续的任务，一个好的向量化表示方法能够表示更多的内涵意义，从而让词包含更多的信息。</p>

<h1 id="toc_0">如何去表示一个字/单词</h1>

<p>在所有NLP任务中，第一个也是最重要的公共问题是我们如何将单词表示为任何模型的输入。早期NLP的许多工作都将单词视为原子符号。为了在大多数NLP任务中表现出色，我们首先需要对单词之间的相似性和差异有一些概念。有了字向量，我们可以很容易解决这些问题(使用距离度量，如jaccard、cosine、eu-clidean等)不管是英文，还是中文都存在大量的单词词汇，这些词汇中间大所存在着或多或少的关系，我们希望可以将这些词映射到某个<code>词空间中</code>作为这个空间中某个点。这样这个点在空间中的向量化表示就可以用来表示这个词了，同时可以在这个空间中可以度量词之间的关系。而最简单的词空间就是整个词汇表空间。</p>

<h1 id="toc_1">One-Hot</h1>

<p>One-Hot是一种非常简单的词向量表示，它使用\(R^{|V|*1}\)大小的向量表示每个单词，其中\(|V|\)表示词汇表大的小，如要表示‘A’:它以词汇表大小n构建一个长度为n的向量[0,0,0,....1...0]，向量中除了单词‘A’所在的位置为1，其他位置都为0.<br/>
<img src="media/15395247337147/15529937344598.jpg" alt="" style="width:489px;"/></p>

<p>但是我们发现这种表示方法把每个单词表示成单独的个体，无法体现出他们之间的联系，而且数据稀疏，空间巨大。因此我们可以将它映射到更小的空间里.</p>

<h1 id="toc_2">基于SVD的向量表示</h1>

<p>SVD:奇异值分解，是一种矩阵分解方法，遍历数据集得到一个共现矩阵，然后进行奇异值分解，选择前K个奇异值，对原始矩阵进行降维，就可以得到一个低维的词向量表示。以下是基于两种共现矩阵的词向量表示。</p>

<h3 id="toc_3">单词-文档共现矩阵</h3>

<p>词-文档的共现矩阵能够表示出<code>相关</code>的词，那么如何用这个矩阵来表示词向量呢，就是以文档的大小作为单词向量的大小，在进行SVD分解得到映射后的向量。假如有M篇文档，有V个词汇，那么这个共现矩阵大小为M*N,而对于新增文档，新增词，这个矩阵就必须更新。</p>

<h3 id="toc_4">单词-单词共现矩阵</h3>

<p>单词-单词的矩阵同样能够表示单词之间的<code>相关性</code>.而单词的量级变化没有文档数变化那么大，相比于单词-文档矩阵，这个矩阵要小很多。采用同样的矩阵分解方式可以得到新的词向量表示。</p>

<p>降维前：<br/>
<img src="media/15395247337147/15530533751931.jpg" alt="" style="width:595px;"/><br/>
降维后：<br/>
<img src="media/15395247337147/15530533964758.jpg" alt="" style="width:565px;"/></p>

<h3 id="toc_5">优势与存在的问题</h3>

<h4 id="toc_6">优势：</h4>

<ul>
<li>有效利用了统计的信息</li>
<li>矩阵分解可以得到部分语义信息</li>
</ul>

<h4 id="toc_7">缺点：</h4>

<ul>
<li>维护的矩阵太大，且非常稀疏(非常多数数据没有共现)</li>
<li>新增数据或单词都要对矩阵进行更新</li>
<li>需要频繁的进行矩阵分解</li>
</ul>

<h3 id="toc_8">解决方案</h3>

<ul>
<li>忽略无用词「无意义」 </li>
<li>使用窗口，即使用文档中单词的距离进行共现计数加权</li>
</ul>

<h1 id="toc_9">基于迭代的词向量-word2vec</h1>

<blockquote>
<p>放弃计算和存储大型的数据矩阵，神经网络兴起之后，我们尝试通过训练一个model来得到单词的词向量，而word2vec的思想就是训练一个参数为词向量的模型,这个模型我们所说的<code>语言模型</code>.当我们得到一个优化的语言模型的时候就得到了一个高质量的词向量(model的隐层参数)。</p>
</blockquote>

<h2 id="toc_10">语言模型（Langue Model）</h2>

<p>语言模型描述的是一个句子(单词序列)的合理性，一个好的语言模型能够给一个合理的句子更高的分数(概率)。加入一个句子s由n个字组成那么他的概率可以表示如下：<br/>
\[<br/>
P_s = P(w_1,w_2...w_n) = \prod _{ i=1 }^{ n }{ P(w_i) } \tag{Unigrams:基于NB条件独立假设}<br/>
\]<br/>
以上是基于条件独立假设的Unigrams模型：假设句子中的每个单词/字相互独立，那么这个句子的概率值就是每个单词/字的概率积，很显然这种假设太过简单，以至于无论几个单词如何组合他们的概率都是相同的。而一句话中每个单词/字的概率是依赖于上下文的，那么句子s的概率可以表示如下：<br/>
\[<br/>
P_s = P(w_1,w_2...w_n) = \prod _{ i=2 }^{ n }{ P(w_i|w_{i-1}) } \tag{Bigrams:基于马尔科夫模型}<br/>
\]<br/>
以上是基于马尔科夫模型的Bigrams:假设当前词的概率依赖于前一个词的概率，那这个句子的概率就是单词、字的条件概率积。这种模型相比于前面的Unigrams提供更多上下文和语序信息。</p>

<blockquote>
<p>以上就是我们所说的<code>N-gram</code>语言模型，N表示当前词的概率依赖于前N-1个词。在这里我们知道了如何去计算了一个句子的概率，那么我们需要如何去训练这个概率呢??「『即如何使得这个概率最大化』」</p>
</blockquote>

<h2 id="toc_11">word2vec两种训练算法：</h2>

<ul>
<li><p>cbow:上下文词袋模型，以上下文词预测这个中心词w。</p></li>
<li><p>skip-gram:以一个中心词W去预测上下文词。</p></li>
</ul>

<p>wordvec的语言模型非常简单，是一个三层的神经网络，包含输入层-隐藏层-输出层如下图：右侧：CBOW,左侧：skip-gram<br/>
<img src="media/15395247337147/15530608575223.jpg" alt="" style="width:282px;"/><img src="media/15395247337147/15530622531387.jpg" alt="" style="width:259px;"/> </p>

<p>参数解析：<hr></p>

<ul>
<li>\(W\):单词</li>
<li>\(V\):词汇表大小</li>
<li>\(N\):我们要得到的词向量的大小</li>
<li>\(k\):第k个中心词</li>
</ul>

<h3 id="toc_12">CBOW模型的训练步骤：</h3>

<ul>
<li>1. 输入的上下文词的one-hot表示。</li>
<li>2. 根据\(W_{V*N}\)输入词矩阵得到上下文词的词嵌入(embding)表示。</li>
<li>3. 将得到的上下文词的词嵌入表示求平均值，得到一个N维的向量。</li>
<li>4. 将这个向量与中心词矩阵\(W&#39;_{N*V}\)相乘，就得到了一个V维的向量，他的每一个值表示的上下文预测的中心词的可能性大小。</li>
<li>5. 为了更好的描述这种可能性大小，我们将其接入softmax，进行归一化城概率值。</li>
<li>6. 将这个概率向量与真正的中心词向量比较就可以得到误差从而更新我们要训练的\(W，W&#39;\)矩阵。</li>
</ul>

<blockquote>
<p>⚠️步骤2中提到得到词嵌入表示，那么这个词嵌入是如何得到的呢？，事实上\(W_{V*N}\)矩阵就是一个<code>词向量矩阵</code>「这个矩阵是由一个均匀分布随机生成的」，而one-hot向量不过是从中选择了属于自己的那个而已。所以：我们在构建一个预训练词嵌入的模型的时候，既可以把词向量作为一个权重参数，也可以直接用词向量去替换成文档的向量表示。</p>
</blockquote>

<p>知道如何训练，那么如何对训练的结果用于优化模型参数呢？需要定义目标函数和损失函数，采用SGD迭代求解的方式不断地去更新参数。</p>

<h2 id="toc_13">损失函数与目标函数</h2>

<p>目标函数:最大化上下文预测的中心词的概率。<br/>
\[\begin{aligned} <br/>
\text {skip-gram}-obj &amp;=\prod_{j=0}^{2 m} P\left(W_{c-m+j} | W_{i}\right) \\ <br/>
\text { context-bow}-obj&amp;=P\left(W_{i} | W_{c-m+j}\right) \end{aligned}<br/>
\]<br/>
m表示上下文窗口大小m=2就是上下文各选2个词</p>

<p>损失函数：交叉熵<br/>
\[\begin{aligned}<br/>
skip-gram H(y,y&#39;)&amp;= -ylog(y&#39;)\\<br/>
&amp;=-1*log(y&#39;) \\<br/>
&amp;=-log{P(W_{c-m...c+m}|W_c)} \\<br/>
&amp;=-log{\prod_{j=0,j\neq m}^{2m}P(W_{c-m+j}|W_c)} \\<br/>
&amp;=-log{\prod_{j=0,j\neq m}^{2m}P(u_{c-m+j}|v&#39;)}\\<br/>
&amp;=-log\prod_{j=0,j\neq m}^{2m}(\frac {exp^{u_{c-m+j}^Tv&#39;}}{\sum_{k=0}^{V}exp^{u_k^Tv&#39;}})\\<br/>
J&amp;=-\sum_{j=0,j\neq m}^{2m} u_{c-m+j}^Tv&#39;+log \sum_{i=0}^{V}exp^{u_i^Tv&#39;}\\<br/>
context-bow H(y,y&#39;)&amp;= -ylog(y&#39;)\\<br/>
&amp;=-1*log(y&#39;) \\<br/>
&amp;=-log{P(W_c|W_{c-m...c+m})} \\<br/>
&amp;=-log{P(u_c|v&#39;)}\\<br/>
&amp;=-log{\frac {exp^{u_c^Tv&#39;}}{\sum_{k=0}^{V}exp^{u_k^Tv&#39;}}}\\<br/>
J&amp;=-u_c^Tv&#39;+log{\sum_{i=0}^{V}{u_i^Tv&#39;}}\\<br/>
\end{aligned}<br/>
\]<br/>
有了损失函数，就可以求解梯度\(▽_{J(\theta)}\)，从而更新我们的参数\(u_c=u_c-α▽_J\)</p>

<h2 id="toc_14">word2vec两种优化算法：</h2>

<ul>
<li>hierarchical softmax：层次化softmax</li>
<li>Negative sampling：负采样</li>
</ul>

<blockquote>
<p>为什么要使用优化算法：原因在于使用softmax去计算概率的时候需要计算单词向量与整个词汇表词向量的点积，计算量非常大，并且与词汇表大小线性相关，同时更新两个词向量矩阵也非常耗时。导致计算缓慢。为了解决这一问题因此提出了优化算法。普遍认为Hierarchical Softmax对低频词效果较好；Negative Sampling对高频词效果较好，向量维度较低时效果更好。</p>
</blockquote>

<h3 id="toc_15">hierarchical softmax</h3>

<p>层次化softmax抛弃softmax求解目标词的概率的方法，构建一颗由词汇表V所有词以词频为权重构建的Huffman树，词频越高路径越短，搜索越快。而每个叶子节点都是一个词汇表中的词w，因此求解目标词w的概率就变成了求这条路径的概率。这里采用左子树编码1,右子树编码为0的规则，而每一次分叉采用sigma函数来计算概率。即P(0)=σ(wx+b),这里0表示正例1，表示负例。那么如果单词w的路径为：01011，则他的概率P=\(P(d|x,\theta)=\prod_{\theta=1}^{\theta=4}\sigma(\theta x+b)\),即每一次分叉的概率积。那么这个概率就是目标词的概率p。根据cbow,目标词的概率应该为1，那么他的误差就是1-p,有了这个p。就有了梯度，就可以进行反向传播，更新参数。<br/>
那么这里的参数就是只有θ，而<code>x</code>是一个向量，这个向量就是上下文词向量的向量和。这里不再去乘以词向量矩阵了。而是直接经过hierarchical softmax计算目标词得概率。</p>

<h3 id="toc_16">Negative sampling</h3>

<p>负采样也是一种优化训练过程的方式，在我们更新参数的时候只有一个向量是跟目标词相关的，那么我们可以在其他的此中随机选择几个进行参数的更新，避免所有的参数更新，大大降低了计算量和空间消耗。。<br/>
假设：<code>NEG(w)</code>：w的负样本。<code>u</code>表示正样本\(w + NEG(w)\),\(L^{w}(\widetilde{w}\):表示的是\(\widetilde{w}\)为正样本和负样本的概率<br/>
\[<br/>
L^{w}(\widetilde{w})=\left\{\begin{array}{ll}{1,} &amp; {\widetilde{w}=w} \\ {0,} &amp; {\widetilde{w} \neq w}\end{array}\right.<br/>
\]<br/>
得到此时的负采样的目标是最大化如下的函数：<br/>
\[<br/>
g(w)=\prod_{u \in\{w\} \cup N E G(w)} p(u | \text { Context }(w))<br/>
\]<br/>
\[<br/>
p(u | \text { Context }(w))=\left[\sigma\left(\mathbf{x}_{w}^{\top} \theta^{u}\right)\right]^{L^{w_{(u)}}} \cdot\left[1-\sigma\left(\mathbf{x}_{w}^{\top} \theta^{u}\right)\right]^{1-L^{w}(u)}<br/>
\]<br/>
得到最终的关于负采样的最大化目标函数：<br/>
\[<br/>
g(w)=\sigma\left(\mathbf{x}_{w}^{\top} \theta^{w}\right) \prod_{u \in N E G(w)}\left[1-\sigma\left(\mathbf{x}_{w}^{\top} \theta^{u}\right)\right]<br/>
\]<br/>
以上表示CBOW的负采样，这个结构清晰地展示我们的目标：<code>最大化正样本概率【左侧】，最小化负样本概率【右侧】</code>。</p>

<p>既然是采样，那么负样本被采样的概率是多少呢？，这里出现了另一个问题，就是次品的不同所带来的采样概率的问题，如何才能得到一个合理的负样本集，其实就是<code>加权采样</code>：<em>用一个线段表示所有样本，分成N段(每一个线段表示一个单词)，线段的长度由词频的0.75次幂决定。</em><br/>
\[<br/>
\operatorname{len}(w)=\frac{[\operatorname{counter}(w)]^{0.75}}{\sum_{u \in \mathcal{D}}[\operatorname{counter}(u)]^{0.75}}<br/>
\]<br/>
我们只要生成0-1之间的随机数，看看落在哪个区间，就能采样到该区间对应的单词了。如果恰巧寻到了自己，则跳过，重新选择。</p>

<h1 id="toc_17">SubSampling</h1>

<blockquote>
<p>SubSampling：降采样，是一种数据的预处理，实际是对高频词进行随机采样，关于随机采样的选择问题，考虑高频词往往提供相对较少的信息，因此可以将高于特定词频的词语丢弃掉，以提高训练速度，同时降低高频词的过度训练，。Mikolov在论文指出这种亚采样能够带来2到10倍的性能提升，并能够提升低频词的表示精度。</p>
</blockquote>

<p>Mikolov论文提到的降采样：p(w)表示单词w被丢弃的概率,t表示阈值，f(w)表示词频<br/>
\[<br/>
P\left(w_{i}\right)=1-\sqrt{\frac{t}{f\left(w_{i}\right)}}<br/>
\]<br/>
Word2vec代码实现：<br/>
\[<br/>
\begin{aligned}<br/>
P\left(w_{i}\right)&amp;=1-\left(\sqrt{\frac{v_{w_{i}}}{\operatorname{sample} * N_{W}}}+1\right) * \frac{\text { sample* } N_{W}}{v_{w_{i}}}\\<br/>
P\left(w_{i}\right)&amp;=1-\left(\sqrt{\frac{\text {sample} * N_{W}}{v_{w_{i}}}}+\frac{\text {sample} * N_{W}}{v_{w_{i}}}\right)<br/>
\end{aligned}<br/>
\]<br/>
其中：NW是参与训练的单词总数，包含重复单词，实质即词频累加；vwi是词wi的词频。在gensim的实现中，对sample&lt;1和sample≥1的情况区分对待。</p>

<h1 id="toc_18">全局词向量表示GLOVE</h1>

<p>glove：</p>

<h1 id="toc_19">参考内容</h1>

<ul>
<li>cs224n2019课程笔记</li>
<li>word2vec论文；
<ul>
<li>1、<a href="">Efficient Estimation of Word Representations in Vector Space</a></li>
<li>2、<a href="">distributed-representations-of-words-and-phrases-and-their-compositionality</a></li>
</ul></li>
<li>word2vec数学原理</li>
<li>一文读懂word2vec</li>
<li><a href="http://qiancy.com/2016/08/17/word2vec-hierarchical-softmax/">http://qiancy.com/2016/08/17/word2vec-hierarchical-softmax/</a></li>
</ul>


</div>

<br /><br />
<hr />

<div class="row clearfix">
  <div class="large-6 columns">
	<div class="text-left" style="padding:15px 0px;">
		
	        <a href="15434622232762.html"  title="Previous Post: 文本分类TextCNN">&laquo; 文本分类TextCNN</a>
	    
	</div>
  </div>
  <div class="large-6 columns">
	<div class="text-right" style="padding:15px 0px;">
		
	        <a href="15560941732047.html" 
	        title="Next Post: 词向量发展历程<二>">词向量发展历程<二> &raquo;</a>
	    
	</div>
  </div>
</div>

<div class="row">
<div style="padding:0px 0.93em;" class="share-comments">

</div>
</div>
<script type="text/javascript">
	$(function(){
		var currentURL = '15395247337147.html';
		$('#side-nav a').each(function(){
			if($(this).attr('href') == currentURL){
				$(this).parent().addClass('active');
			}
		});
	});
</script>  
</div></div>


<div class="page-bottom">
  <div class="row">
  <hr />
  <div class="small-9 columns">
  <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
  <div class="small-3 columns">
  <p class="copyright text-right"><a href="#header">TOP</a></p>
  </div>
   
  </div>
</div>

        </section>
      </div>
    </div>
    
    
    <script src="asset/js/foundation.min.js"></script>
    <script src="asset/js/foundation/foundation.offcanvas.js"></script>
    <script>
      $(document).foundation();

     
    </script>
    
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>

  </body>
</html>
