<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[不以刷题为目的的面试不是个好程序员]]></title>
  <link href="http://atlasbl.cn/coding_for_interview/atom.xml" rel="self"/>
  <link href="http://atlasbl.cn/coding_for_interview/"/>
  <updated>2019-01-29T20:13:50+08:00</updated>
  <id>http://atlasbl.cn/coding_for_interview/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im">MWeb</generator>

  
  <entry>
    <title type="html"><![CDATA[HMM到中文分词]]></title>
    <link href="http://atlasbl.cn/coding_for_interview/15446854936789.html"/>
    <updated>2018-12-13T15:18:13+08:00</updated>
    <id>http://atlasbl.cn/coding_for_interview/15446854936789.html</id>
    <content type="html"><![CDATA[
<p><img src="media/15446854936789/Merkov_2.gif" alt="Merkov_2"/> <br/>
HMM隐马尔科夫模型一直是个很困扰，感觉上又很难的点，这是一篇个人理解的大白话文，去记录HMM是如何用于中文分词的。上面这张图展示的就是一个序列生成的过程，也是一个马尔科夫决策过程。这里说的是<code>「马尔科夫」</code>并不是<code>「隐马尔科夫」</code>。</p>

<h1 id="toc_0">MM(马尔科夫)到HMM(隐马尔科夫)</h1>

<p>上图中有两个状态：R，S,R状态可以变转变为S状态，R也可以转变为R，S也可以转变为S，加入一开始我们以某种概率选择一种状态R或者S，然后以这个状态为初始不断地选择下一个状态，这样就形成了一个观察到的状态序列，我们叫做<strong>观测序列</strong>。我们发现这个观测序列中的每一个状态都是状态集中的一种。那同样的当我们没办法直接观察到原始状态的时候，我们就说我们想得到的<strong>状态序列</strong>是隐藏的。这就是「HMM(隐马尔科夫)」。而我们能观测到的观测序列是另一种状态，比如说r,s。简单来说：</p>

<pre class="line-numbers"><code class="language-text"> MM:(R,S)——&gt;R,R,S,S,S,S,R,S....状态==观测
 
HMM:(R,S)——&gt;?,?,?,?,?,?,?,?....状态(隐)
            r,s,r,s,s,s,r,s....观测
</code></pre>

<p>这就是MM和HMM的区别。</p>

<h1 id="toc_1">HMM中不得不说的五元组</h1>

<p>开始的图中已经提到了<code>状态集</code>，<code>观测集</code>，而生成不同的序列取决于各个状态之间的<code>转移概率矩阵</code>,上图就是以如下的转移矩阵进行状态转换的：</p>

<table>
<thead>
<tr>
<th>转移概率</th>
<th>R</th>
<th>S</th>
</tr>
</thead>

<tbody>
<tr>
<td>R</td>
<td>0.9</td>
<td>0.1</td>
</tr>
<tr>
<td>S</td>
<td>0.1</td>
<td>0.9</td>
</tr>
</tbody>
</table>

<p>当无法观测到状态序列的时候，状态序列和观测序列之间就存在一种概率关系，即：当观测为r时，状态为s的概率。这种概率关系计做<code>发射概率矩阵</code>如下：</p>

<table>
<thead>
<tr>
<th>发射概率</th>
<th>r</th>
<th>s</th>
</tr>
</thead>

<tbody>
<tr>
<td>R</td>
<td>0.8</td>
<td>0.2</td>
</tr>
<tr>
<td>S</td>
<td>0.1</td>
<td>0.9</td>
</tr>
</tbody>
</table>

<p>而最后一个就是<code>初始状态分布</code>,他决定了序列的开始，而开始总是结果有着一定的影响，那么他到底是如何影响的呢，这就与HMM的数学假设有关了。</p>

<h1 id="toc_2">HMM的三个基本假设</h1>

<p>数学中经常会用到很多的假设，因为在真实环境中影响问题的因素多种多样，假设通常用来简化问题，从而找到问题的解法。否则很多问题将无法求解。</p>

<ul>
<li><strong>有限历史性假设</strong>：当前的状态取决于他前那一刻状态，这就为什么初始分布会对对状态序列有影响了。</li>
<li><strong>输出独立性假设</strong>：输出仅与当前状态有关，换句话说：观测序列当前值仅与其对应的状态序列当前值有关，再简单点来说：t时刻观测到r还是s,仅取决于t时刻状态时R还是S。</li>
<li><strong>齐次性假设</strong>：每个时刻的状态取决于前一刻的状态，而与时间没有关系。简单来说：状态不受时间影响。
有了假设，有了HMM核心的五元组就可以解决三个问题，这其中就包括中文分词。</li>
</ul>

<h1 id="toc_3">HMM解决的三大问题</h1>

<ul>
<li><strong>1、求解观察值序列</strong>：给定HMM其他的四元组求解一个观察序列的概率，</li>
<li><strong>2、求解状态值序列</strong>：给定HMM其他的四元组求解一个最有可能生成观测序列的状态序列。 如：<code>中文分词</code>，<code>语音识别</code>，采用<code>Viterbi</code>算法</li>
<li><strong>3、学习HMM模型</strong>：已知观测序列，去学习HMM模型。</li>
</ul>

<h1 id="toc_4">中文分词的解决方案</h1>

<p>如上所述：中文分词就是HMM模型+一句话(观测序列)+viterbi算法去求解状态值序列。而中文分词的状态值序列就是一个状态集（B,E,M,S）的序列，S:单字成词，B:一个词的开头，E:一个词的结尾，M：一个词的中间。</p>

<blockquote>
<p>比如说：<code>我喜欢秦皇岛的阳光</code>这句话，我们得到一个最可能的状态序列：SBEBMESBE.这个状态序列表示的就是分词的结果，因此得到的分词结果就是：<code>我/喜欢/秦皇岛/的/阳光</code></p>
</blockquote>

<p>这里面有两个问题：</p>

<ul>
<li>HMM模型怎么来的？-&gt;预训练模型</li>
<li>如何求解状态序列？-&gt;&#39;viterbi算法&#39;</li>
</ul>

<p>也就是说我们的分词算法调用一个已经训练好的HMM模型加上‘viterbi算法’就可以实现中文分词,如下图：<img src="media/15446854936789/cutchinese.png" alt="cutchinese"/></p>

<h1 id="toc_5">中文分词的Viterbi算法</h1>

<blockquote>
<p>Viterbi算法是一种<code>动态规划算法</code>，动态规范算法通常是用来求解最优化问题，而在中文分词中则用来求解最大化观测序列概率的状态序列。也就是说求解一个状态序列使得到的观测序列无限的接近于我们看到的序列。</p>
</blockquote>

<p>以上面的<strong>「我喜欢秦皇岛的阳光」</strong>为例，只有当我们生成的序列是「SBEBMESBE」的时候，生成<strong>『我喜欢秦皇岛的阳光』</strong>句子的概率才达到最大。此时『SBEBMESBE』就是我们要求的状态序列。为了方便后面的描述和代码下面更加精确的定义五元组：</p>

<ol>
<li>状态集：O(B,S,E,M)</li>
<li>观测集：S(我，喜，欢，秦，皇，岛，的，阳，光)</li>
<li>初始概率分布：PS(0.4,0.6,0,0)</li>
<li><p>转移概率矩阵:当前时刻状态为某个值的时候下一刻状态为某个值得概率</p>
<table>
<thead>
<tr>
<th></th>
<th>S</th>
<th>B</th>
<th>M</th>
<th>E</th>
</tr>
</thead>
<tbody>
<tr>
<td>S</td>
<td>0.5</td>
<td>0.5</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>B</td>
<td>0</td>
<td>0</td>
<td>0.5</td>
<td>0.5</td>
</tr>
<tr>
<td>M</td>
<td>0</td>
<td>0</td>
<td>0.5</td>
<td>0.5</td>
</tr>
<tr>
<td>E</td>
<td>0.4</td>
<td>0.6</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table></li>
<li><p>发射概率矩阵:状态为某个值的时候观测到某个值得概率</p>
<table>
<thead>
<tr>
<th></th>
<th>我</th>
<th>喜</th>
<th>欢</th>
<th>秦</th>
<th>皇</th>
<th>导</th>
<th>的</th>
<th>阳</th>
<th>光</th>
</tr>
</thead>
<tbody>
<tr>
<td>S</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>B</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>M</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>E</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table></li>
</ol>

<p>有了如上的五元组，我们就可以用Viterbi算法解出最可能的状态序列。以下是jieba分词中的Viterbi实现，参数就是五元组，</p>

<pre class="line-numbers"><code class="language-python">&#39;&#39;&#39;jieba分词中的Viterbi实现&#39;&#39;&#39;
def viterbi(obs, states, start_p, trans_p, emit_p):
    V = [{}]  # tabular
    path = {}
    for y in states:  # init
        V[0][y] = start_p[y] + emit_p[y].get(obs[0], MIN_FLOAT)
        path[y] = [y]
    for t in xrange(1, len(obs)):
        V.append({})
        newpath = {}
        for y in states:
            em_p = emit_p[y].get(obs[t], MIN_FLOAT)
            (prob, state) = max(
                [(V[t - 1][y0] + trans_p[y0].get(y, MIN_FLOAT) + em_p, y0) for y0 in PrevStatus[y]])
            V[t][y] = prob
            newpath[y] = path[state] + [y]
        path = newpath

    (prob, state) = max((V[len(obs) - 1][y], y) for y in &#39;ES&#39;)
    return (prob, path[state])
</code></pre>

<blockquote>
<p>核心部分就是从第一个时刻开始(即顺序遍历观测集)，对于每一个时刻(每一个观测值)计算出得到这个观测值最大概率P,和对应的状态值，有了这个概率和状态值，就可以求得下一时刻的最大概率P和对应的状态值，知道遍历完这个观测值序列，我们就得到了一个值得最终概率最大的状态值序列，而这个序列就是我们所求的。</p>
</blockquote>

<h1 id="toc_6">其他问题</h1>

<p>当状态值是『B S E M』的时候，HMM可以用来做中文分词，同样的当状态值是『n,v,...』等词性的时候，HMM就可以用来解决词性标注的问题。同样的命名实体识别的问题也可以这样解决。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[文本分类TextCNN]]></title>
    <link href="http://atlasbl.cn/coding_for_interview/15434622232762.html"/>
    <updated>2018-11-29T11:30:23+08:00</updated>
    <id>http://atlasbl.cn/coding_for_interview/15434622232762.html</id>
    <content type="html"><![CDATA[
<p>about:<a href="https://arxiv.org/abs/1408.5882">《Convolutional Neural Networks for Sentence Classification》</a><br/>
TextCNN即使用卷积神经网络来实现文本分类，CNN经常被用来做图像的分类，而图像在转化成像素点的时候，就变成了一个三维(彩色图片/三通道)的数字矩阵，同样的文本数据在向量化之后也是向量矩阵不过是个一维的/二维的(每个数据的维度取决于向量化的方式)，既然都是数值矩阵，那么是不是也可以像图像一样使用CNN来做呢，这就是TXTCNN要做的事情。</p>

<h1 id="toc_0">TextCNN的网络结构</h1>

<p><img src="media/15434622232762/15487625443034.jpg" alt="" style="width:615px;"/></p>

<p>上图非常清晰的展示了textcnn的整体结构，核心在于4个尺寸的卷积核,对输入数据进行卷积操作之后MaxPool+Flatten操作之后，得到四分数据（一个卷积核一份数据），用一个concat层将四分数据拼接成一份。在接dropout层之后接Dense全连接层进行非线性变换，最后接一层Dense层使用softmax激活函数进行输出分类。</p>

<h1 id="toc_1">TextCNN的优点</h1>

<p>1、首先作为深度学习模型，可以有效的运用预训练词向量带来效果的提升，相比于原始的文本向量化有很大优势<br/>
2、CNN结构本身相比于其他的序列模型在训练上存在速度优势，TextCNN同样。<br/>
3、TextCNN中采用的卷积核长度为词向量维度，多种宽度的组合，等同于N-gram操作，一定程度上保留了词之间的位置特征。</p>

<h1 id="toc_2">TextCNN的keras实现</h1>

<pre class="line-numbers"><code class="language-python">
num_words = 200000
maxlen = 1000  # 1000-&gt;2000
max_features = 200000
embed_size = 200  # 100
num_classes = 20
batch_size = 100
epochs = 100
filter_sizes = [2, 3, 4, 5]  # +6,7
drop_late = 0.1

def build_cnn():
    # Inputs
    input_seq = Input(shape=[maxlen], name=&#39;input_seq&#39;)
    # Embeddings layers
    emb_comment = Embedding(num_words, embed_size, embeddings_initializer=&#39;uniform&#39;,trainable = True)(input_seq)

    # conv layers
    convs = []
    for ks in filter_sizes:
        l_conv = Conv1D(filters=embed_size, kernel_size=ks, activation=&#39;relu&#39;)(emb_comment)
        l_pool = MaxPooling1D(maxlen - ks + 1)(l_conv)
        l_pool = Flatten()(l_pool)
        convs.append(l_pool)
    merge = concatenate(convs, axis=1)

    drop = Dropout(drop_late)(merge)
    Ds1 = Dense(64, activation=&#39;relu&#39;)(drop)
    output = Dense(num_classes, activation=&#39;softmax&#39;)(Ds1)
    model = Model([input_seq], output)
    model.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=&quot;nadam&quot;, metrics=[&#39;acc&#39;])
    # 画出模型结构
    plot_model(model, to_file=&#39;{}.png&#39;.format(path_model))
    # 输出模型结构
    print(model.summary())
    return model

def fit_cnn(model, X, y):
    &quot;&quot;&quot;&quot;&quot;&quot;
    # callback
    early_stopping = EarlyStopping(monitor=&#39;val_loss&#39;,
                                   patience=5)
    checkpoint = ModelCheckpoint(path_model,
                                 monitor=&#39;val_acc&#39;,
                                 verbose=1,
                                 save_best_only=True,
                                 mode=&#39;max&#39;)
    TB_plot = TensorBoard(log_dir=&#39;data/logs&#39;,
                          batch_size=100,
                          write_images=True)
    # fit
    hist = model.fit(X, y, validation_split=0.2,
                     batch_size=batch_size,
                     epochs=epochs,
                     shuffle=True,
                     callbacks=[early_stopping, checkpoint, TB_plot])

    bst_fit_loss = min(hist.history[&#39;loss&#39;])
    bst_fit_acc = max(hist.history[&#39;acc&#39;])

    bst_val_loss = min(hist.history[&#39;val_loss&#39;])
    bst_val_acc = max(hist.history[&#39;val_acc&#39;])

    print(
        &quot;bst_fit_loss:{}\nbst_fit_acc:{}\nbst_val_loss:{}\nbst_val_acc:{}&quot;.format(bst_fit_loss, bst_fit_acc,
                                                                                  bst_val_loss,
                                                                                  bst_val_acc))
    return model
</code></pre>

<h1 id="toc_3">调参</h1>

<p>参照：<a href="https://arxiv.org/pdf/1510.03820.pdf">&lt;<A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification>&gt;</a>textcnn调参指南。<br/>
其实可以调整的参数不多，主要是卷积核的组合，dropout的失活率，其实更多的提升还是来自于对文本的处理和业务、任务的理解上。结合预训练词向量。</p>

<h1 id="toc_4">总结：</h1>

<p>TextCNN算是文本分类里面最简单也最易理解的模型，因为本身cnn就比较好理解，主要是在于理解卷积操作，在图像和文本上的差异，图像上卷积核更灵活，文本上卷积核长度必须是词向量长度，因为卷积核最小也必须覆盖一个字符。否则就会把一个字符的词向量拆开。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Keras中的数据预处理「文本」]]></title>
    <link href="http://atlasbl.cn/coding_for_interview/15429415402935.html"/>
    <updated>2018-11-23T10:52:20+08:00</updated>
    <id>http://atlasbl.cn/coding_for_interview/15429415402935.html</id>
    <content type="html"><![CDATA[
<p>英文文本默认的是空格分割，不像中文以字成词组词成句，中间连贯。所以中文相较于英文来说就多了分词这一步，分词之后得到跟英文一样的格式就可以作为模型的输入数据了。<br/>
示例：</p>

<pre class="line-numbers"><code class="language-python">EN;
    I am chinese.
ZH:
    我是中国人。=&gt; 我 是 中国 人
</code></pre>

<p>分词部分使用中文分词器就可以实现。当然我们也可以采用其他的分隔符，如<code>-</code>,<code>|</code>,<code>&gt;</code>,只要不语文本内容冲突就可以。如果我们有N条文本，那么就得到一个N行的分词后的序列，如下：</p>

<pre class="line-numbers"><code class="language-python">0   我 是 中国 人
1   我 是 中国 人
2   我 是 中国 人
... 我 是 中国 人
N   我 是 中国 人
</code></pre>

<p>中文文本深度学习建模流程：</p>

<pre class="line-numbers"><code class="language-mermaid">graph LR;
  中文处理--&gt;数据处理;
  数据处理--&gt;模型训练;
  模型训练--&gt;模型优化;
</code></pre>

<h1 id="toc_0">Keras中的文本处理</h1>

<pre class="line-numbers"><code class="language-python">from keras.preprocessing.text import Tokenizer,sequence 
from keras.utils import np_utils
</code></pre>

<p>数据操作：</p>

<pre class="line-numbers"><code class="language-python"># 词元分割，数值化与选择
tokenizer = Tokenizer(num_words=num_words)
tokenizer.fit_on_texts(dt_train[column])

# 对每一段话转换为数字列表，使用每个词的编号进行编号
x_train_seq = tokenizer.texts_to_sequences(dt_train[column])
x_test_seq = tokenizer.texts_to_sequences(dt_test[column])

# 序列截断于填充
x_train = sequence.pad_sequences(x_train_seq, maxlen=maxlen)
x_test = sequence.pad_sequences(x_test_seq, maxlen=maxlen)

# 类别编码
y_train = np_utils.to_categorical(dt_train[&#39;class&#39;], num_classes=num_classes)
</code></pre>

<h2 id="toc_1">Tokenizer：向量化</h2>

<p>向量化文本预料，用于将文本转成整形的数值序列，他将预料数据按照特定的分隔符切分，并统计词频相关信息，构建词的序列词典，然后将每一条文本</p>

<pre class="line-numbers"><code class="language-python">Tokenizer(num_words=None,                                # 选择多少个词（以词频排序选TOP num_words个）
         filters=&#39;!&quot;#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n&#39;, # 过滤的字符，默认是英文标点符号
         lower=True,                                     # 是否转小写(True时候是英文)
         split=&#39; &#39;,                                      # 语料的分隔符，默认是空格，可以自行修改
         char_level=False,                               # 是否进行字符级别的分隔
         oov_token=None,)
fit_on_texts(texts)                                      # 构建词典，并统计词频，排序
texts_to_sequences(texts)                                # 文本转换成索引序列「仅包含在TOP num_words的词会进行序列转换」
</code></pre>

<h2 id="toc_2">sequence:序列修整</h2>

<p>文本长度参差不一，所以得到的向量也是不一样长的，必须将他们规整成相同长度的向量，长的截断，短的填充。</p>

<pre class="line-numbers"><code class="language-python">sequence.pad_sequences(sequences, 
                       maxlen=None,                       # 截取的最大长度，也是向量的最大长度
                       dtype=&#39;int32&#39;,
                       padding=&#39;pre&#39;,                     # pre:截取前面 post:截取后面，
                       truncating=&#39;pre&#39;, value=0.)        # pre:前面填充 post:后面填充，
</code></pre>

<h2 id="toc_3">to_categorical:类别编码</h2>

<p>把<code>「数值型」</code>的类别变量编码成shape=(n,)的类别向量，简单来说就是one_hot，但是他只支持数值型输入。</p>

<pre class="line-numbers"><code class="language-python">np_utils.to_categorical(y, num_classes=None)               # num_classes表示类别数，y表示标签列   
如下：
0 1
1 2
2 3
3 2
==&gt;
0 1 0 0
1 0 1 0
2 0 0 1
3 0 1 0
</code></pre>

<h1 id="toc_4">整个处理流程</h1>

<pre class="line-numbers"><code class="language-python">def processing(dt_train, dt_test):
    tokenizer = Tokenizer(num_words=num_words)
    tokenizer.fit_on_texts(dt_train[column])
    # 对每一段话转换为数字列表，使用每个词的索引进行编号
    x_train_seq = tokenizer.texts_to_sequences(dt_train[column])
    x_test_seq = tokenizer.texts_to_sequences(dt_test[column])

    x_train = sequence.pad_sequences(x_train_seq, maxlen=maxlen)
    x_test = sequence.pad_sequences(x_test_seq, maxlen=maxlen)

    y_train = np_utils.to_categorical(dt_train[&#39;class&#39;], num_classes=num_classes)
    return x_train,x_test,y_train
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[深度学习库keras]]></title>
    <link href="http://atlasbl.cn/coding_for_interview/15421840556162.html"/>
    <updated>2018-11-14T16:27:35+08:00</updated>
    <id>http://atlasbl.cn/coding_for_interview/15421840556162.html</id>
    <content type="html"><![CDATA[
<pre class="line-numbers"><code class="language-mermaid">graph LR;
  数据处理--&gt;模型训练;
  模型训练--&gt;模型优化;
</code></pre>

<p>深度学习整个的流程跟机器学习没什么差别，主要的区别在模型上，深度神经网络更复杂，更加多变，能力更强，「 简单快读的入门方式就是找到体系的流程快读的实现demo。而这个流程的一些问题和阻碍，以及应该注意的地方是我这里想说的。」</p>

<h1 id="toc_0">keras相关的模块</h1>

<p>涉及到机器学习总是离不开几个关键词<code>『数据』</code>，<code>『模型』</code>，<code>『评估函数』</code>，<code>『损失函数』</code>。同样的深度学习也以这几个关键词为基础。keras以主流的深度学习库为后端构建了简单易用的上层API以便我们使用。包括数据处理，评估函数，损失函数的实现，更是提供了各种网络层的实现和封装，通过简单的函数API和序列API进行各种网络层的堆叠就可以实现各式各样的神经网络结构。</p>

<h2 id="toc_1">keras的预处理模块</h2>

<p><code>from keras.preprocessing import sequence,text,image</code></p>

<ul>
<li>image：图像相关</li>
<li>sequence：序列相关</li>
<li>text：文本相关</li>
</ul>

<h2 id="toc_2">keras的Utils</h2>

<p>keras的utils模块主要是提供建模流程的各种辅助操作，其中包括数据相关的，IO相关的，数据操作相关，网络层相关的操作具体如下：</p>

<pre class="line-numbers"><code class="language-python">from .io_utils import HDF5Matrix
from .data_utils import get_file
from .data_utils import Sequence
from .data_utils import GeneratorEnqueuer
from .data_utils import OrderedEnqueuer
from .generic_utils import CustomObjectScope
from .generic_utils import custom_object_scope
from .generic_utils import get_custom_objects
from .generic_utils import serialize_keras_object
from .generic_utils import deserialize_keras_object
from .generic_utils import Progbar
from .layer_utils import convert_all_kernels_in_model
from .layer_utils import print_summary
from .vis_utils import plot_model
from .np_utils import to_categorical
from .np_utils import normalize
from .multi_gpu_utils import multi_gpu_model
</code></pre>

<p>而常用的主要是：<code>from keras.utils import np_utils, plot_model</code></p>

<ul>
<li>np_utils：
<ul>
<li>to_categorical：进行类别编码</li>
<li>normalize：正则化</li>
</ul></li>
<li>plot_model:用与画出模型结构</li>
</ul>

<h2 id="toc_3">keras的Models</h2>

<p>keras的models模块主要是实现模型相关的操作：</p>

<ul>
<li>load_model：模型加载，可以加在已经保存的训练好的模型直接用于预测</li>
<li>save_model：模型保存，可以讲训练好的模型进行保存</li>
<li>Sequential：顺序模型，可以直接使用此函数构建一个模型网络，顺序模型是多个网络层的线性堆叠。</li>
</ul>

<pre class="line-numbers"><code class="language-python">from keras.models import Sequential
from keras.layers import Dense, Activation
model = Sequential([
    Dense(32, input_shape=(784,)),
    Activation(&#39;relu&#39;),
    Dense(10),
    Activation(&#39;softmax&#39;),
])
&#39;&#39;&#39;或&#39;&#39;&#39;
model = Sequential()
model.add(Dense(32, input_dim=784))
model.add(Activation(&#39;relu&#39;))
model.add(Dense(10))
model.add(Activation(&#39;softmax&#39;))
</code></pre>

<h2 id="toc_4">keras的losses</h2>

<p>losses即损失函数，也可以叫做目标函数，优化评分函数，它用来告诉我们模型该如何调整到最优，keras中的losses 实现如下：</p>

<pre class="line-numbers"><code class="language-python">回归
    mse = MSE = mean_squared_error
    mae = MAE = mean_absolute_error
    mape = MAPE = mean_absolute_percentage_error
    msle = MSLE = mean_squared_logarithmic_error
    kld = KLD = kullback_leibler_divergence
    cosine = cosine_proximity
分类
    categorical_crossentropy：多分
    binary_crossentropy：二分
    sparse_categorical_crossentropy
。。。
</code></pre>

<p>对于不同的问题需要选择不同的损失函数，二分勒，多分类，回归都需要不同的损失函数，模型才能正常训练。</p>

<h2 id="toc_5">keras的metrics</h2>

<p>评估函数是用来评估模型效果，与loss有很多公用的函数，keras中的metrics实现如下：</p>

<pre class="line-numbers"><code class="language-python">误差：
    mse = MSE = mean_squared_error
    mae = MAE = mean_absolute_error
    mape = MAPE = mean_absolute_percentage_error
    msle = MSLE = mean_squared_logarithmic_error
    cosine = cosine_proximity
准确率：
    binary_accuracy:
    categorical_accuracy
    sparse_categorical_accuracy
。。。
</code></pre>

<blockquote>
<p>如果没有我们需要的评价函数的话就需要自定义评价函数如：f1_score</p>
</blockquote>

<pre class="line-numbers"><code class="language-python">def f1_score(y_true, y_pred):
    # Count positive samples.
    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))
    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))
    # If there are no true samples, fix the F1 score at 0.
    if c3 == 0:
        return 0
    # How many selected items are relevant?
    precision = c1 / c2
    # How many relevant items are selected?
    recall = c1 / c3
    # Calculate f1_score
    f1_score = 2 * (precision * recall) / (precision + recall)
    return f1_score
</code></pre>

<h2 id="toc_6">keras的「optimizer」优化器</h2>

<p>optermizer优化器是用来根据误差不断调整参数使得目标函数预测值不断接近真实值的一种计算方法。耳熟能详的<code>梯度下降</code>就是其一，其他的还有牛顿法，共轭梯度法，keras中的优化器如下：</p>

<pre class="line-numbers"><code class="language-python">sgd = SGD
rmsprop = RMSprop
adagrad = Adagrad
adadelta = Adadelta
adam = Adam
adamax = Adamax
nadam = Nadam
</code></pre>

<h2 id="toc_7">keras的callback</h2>

<p>callback回调函数是用于在模型训练过程中对模型活着数据进行操作，最常用的是：</p>

<pre class="line-numbers"><code class="language-python">ModelCheckpoint:训练的过程中不断的保存模型，是一种容错机制。
EarlyStopping:提前停止，一种降低训练时间和过拟合的策略。
</code></pre>

<h2 id="toc_8">keras的layers「网络层」</h2>

<p>0、嵌入层</p>

<pre class="line-numbers"><code class="language-python">Embedding:
</code></pre>

<p>1、核心层：</p>

<pre class="line-numbers"><code class="language-python">Input:输入层，用于实例化Keras张量。
Dense：全链接层
Activation：激活函数
Dropout：随机失活，防止过拟合
Faltten:输入展平
Reshape：修改尺寸
Permute：纬度置换
RepeatVector：重复输入数据
Lambda：将表达式封装为一个层，即自定义层
ActivityRegularization：正则化
Masking：
SpatialDropout1D
SpatialDropout2D
SpatialDropout3D
</code></pre>

<p>2、卷积层：构成卷积神经网络的基础</p>

<pre class="line-numbers"><code class="language-python">Conv1D:一维卷积
Conv2D:二维卷积
Conv3D:三维卷积

UpSampling1D:1D 输入的上采样层。
UpSampling2D:2D 输入的上采样层。
UpSampling3D:3D 输入的上采样层。
。。。
</code></pre>

<p>3、循环层：循环神经网路的基础</p>

<pre class="line-numbers"><code class="language-python">RNN 
SimpleRNN
GRU
Lstm 
</code></pre>

<p>4、池化层：</p>

<pre class="line-numbers"><code class="language-python">最大池化层
    MaxPooling1D
    MaxPooling2D
    MaxPooling3D
平均池化层：
    AveragePooling1D
    AveragePooling2D
    AveragePooling3D
全局最大池化层
    GlobalMaxPooling1D
    GlobalMaxPooling2D
    GlobalMaxPooling3D
全局平均池化层：
    GlobalAveragePooling1D
    GlobalAveragePooling2D
    GlobalAveragePooling3D
</code></pre>

<p>5、融合层：两个/多个层的数据进行某种计算进行融合</p>

<pre class="line-numbers"><code class="language-python">Add:逐元素相加
Subtract:逐元素相减
Multiply：逐元素相乘
Average：逐元素平均
Maximum：逐元素最大值
Concatenate：按轴拼接
Dot：点积
</code></pre>

<p>6、噪声层：缓解过拟合</p>

<pre class="line-numbers"><code class="language-python">GaussianNoise:高斯噪声
GaussianDropout:
AlphaDropout:
</code></pre>

<p>7、层封装器</p>

<pre class="line-numbers"><code class="language-python">TimeDistributed:
Bidirectional:RNN的双向封装器，对序列进行前向和后向计算。
</code></pre>

<p>8、激活函数：用于对数据进行非线性变换</p>

<pre class="line-numbers"><code class="language-python">sigmoid
softmax
hard_sigmoid
linear:线性激活函数（即不做任何改变）
elu:指数线性单元。
selu:可伸缩的指数线性单元（SELU）。
relu:线性修正单元。
tanh:双曲正切激活函数。
softplus:log(exp(x) + 1)
softsign:x / (abs(x) + 1)
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[想对自己说的话]]></title>
    <link href="http://atlasbl.cn/coding_for_interview/15406404520470.html"/>
    <updated>2018-10-27T19:40:52+08:00</updated>
    <id>http://atlasbl.cn/coding_for_interview/15406404520470.html</id>
    <content type="html"><![CDATA[
<p>「 25岁了，也许这是注定平凡的一生，但这又有什么关系，没有谁来到世上注定是伟大的。我也不过是这平凡的大多数中的一员。」</p>

<p>「 不要总是否定自己，其实你已经超越了很多人，至少在走自己的路，也不要总是把目标定的那么高远，总觉得，过去了那么久好像自己还是那个样子，其实你已经成长了很多，真的。」</p>

<p>「 世界那么大，多去看看。」</p>

<p>「 阶段性的目标，学到自己想学到的东西就够了。」</p>

<p>「 但行好事，莫问前程。」</p>

<p>「 无钱莫入众，言轻莫劝人，待到功成后，把酒言初心。」</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[最近想做的事]]></title>
    <link href="http://atlasbl.cn/coding_for_interview/15392402403847.html"/>
    <updated>2018-10-11T14:44:00+08:00</updated>
    <id>http://atlasbl.cn/coding_for_interview/15392402403847.html</id>
    <content type="html"><![CDATA[
<ul>
<li>First：刷题，刷题，刷题，整理好这个网站，做好知识积累</li>
<li>Second：做一个爬虫，爬取各个数据科学比赛网站的赛题，做分类，英文的做翻译，进行内容规则提取，结构化的在一个网页中展示出来，</li>
<li>Third：NLP内容的学习，深度学习内容的学习</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[我的偶像]]></title>
    <link href="http://atlasbl.cn/coding_for_interview/15392398597900.html"/>
    <updated>2018-10-11T14:37:39+08:00</updated>
    <id>http://atlasbl.cn/coding_for_interview/15392398597900.html</id>
    <content type="html"><![CDATA[
<p>「用偶像中这个词可能不太确切，但是好像偶像更能让我像个年轻人一样仿佛对生活充满了热情。」</p>

<h1 id="toc_0"><span style="color:rgb(0,173,167)">「Andrew Ng」</span></h1>

<p>吴恩达（英语：Andrew Ng，1976年－）是斯坦福大学计算机科学系和电气工程系的副教授，斯坦福人工智能实验室的主任。他还与达芙妮·科勒一起创建了在线教育平台Coursera。</p>

<h1 id="toc_1"><span style="color:rgb(0,173,167)">「唐骏」</span></h1>

<p>唐骏，是中国的著名职业经理人，曾留学日本和美国，有“打工皇帝”之称。他本科毕业于北京邮电大学，后前往日本名古屋大学深造取得工学硕士学位。1994年加入微软公司美国总部，先后担任微软全球技术中心总经理，微软中国公司总裁。2004年出任中国最大的互动娱乐公司盛大网络公司总裁，并帮助盛大公司在美国纳斯达克成功上市，被华尔街誉为中国资本的第一人。2008年唐骏先生以“十亿“身价转会新华都集团出任总裁兼CEO。</p>

<h1 id="toc_2"><span style="color:rgb(0,173,167)">「陈天桥」</span></h1>

<p>陈天桥，1973年5月生于浙江新昌。盛大网络董事会主席和首席执行官（CEO）。陈天桥是中国网络游戏产业的奠基人和领军人物，缔造了一个白手起家创业的神话，其影响力遍布国内及全球。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[吴翼：我的ACM参赛故事]]></title>
    <link href="http://atlasbl.cn/coding_for_interview/15391808991416.html"/>
    <updated>2018-10-10T22:14:59+08:00</updated>
    <id>http://atlasbl.cn/coding_for_interview/15391808991416.html</id>
    <content type="html"><![CDATA[
<p><a href="https://www.cwzj.com/archives/1451">source: 吴翼：我的ACM参赛故事 | 我爱计算机 </a><br/>
作者：吴翼，曾经两次拿过ACM/ICPC(国际大学生程序设计竞赛)的奖牌，保送清华大学姚班，毕业后前往加州伯克利攻读PHD，师从人工智能大神Stuart Russell。本文系转载。</p>

<p>从我接触程序竞赛到现在应该有十多年了，单说ACM竞赛，从第一次非正式参赛到现在也差不多有7年多的样子。有太多的故事，想说的话，却一直没能有机会写下来。一方面是自己忙，一方面也是自己懒。所以很感谢能有人“逼”我来写点什么。想到会有很多人来读我写的文字，自己也觉得很开心。其实每个人的生活都是一部电影，只是没有那么多幸运的人有机会去诉说。这里，且说三个小故事。</p>

<p>一个关于仰望，崇拜和梦想的故事</p>

<p>2000年，有一个天才的高中生，在IMO（国际数学奥林匹克竞赛）中以满分获得了金牌并提前一年进入北大读书。在他前往北大之前，他受邀到他毕业的小学给了一个演讲。而听众里面有一个三年级的小正太，演讲啥也没记住，倒是在心里埋下了一个小小的梦想：“我也想代表中国去拿金牌”。</p>

<p>10年后这个天才高中生博士毕业，并受邀回到他毕业的高中又给了一个演讲，当年的小正太变成了少年，又恰好在听众席里。这个少年刚刚入选了信息学奥林匹克中国国家队，将代表中国去加拿大参加国际比赛。</p>

<p>这个天才高中生叫恽之玮，拉马努金奖得主，目前在斯坦福大学任教；而那个小正太，也就是那个少年，后来非常丢人的拿到了国际比赛的银牌，又在ACM竞赛圈子里晃悠了一圈，现在也跑到美国读博士了。很不幸，世界没能狗血到再度产生一个莫扎特和贝多芬的故事。少年自然没法和天才相比拟，不过起码在需要写写文字的时候，少年的故事可以这样开头：“我有一个梦想”。</p>

<p>小学的时候参加计算机兴趣小组，从那里接触了编程。不过，要说真正被带入了程序竞赛这条不归路，那还得归功于我的恩师，常州高级中学的曹文老师的顶级忽悠能力。</p>

<p>我从初一开始跟着曹老师学习信息学竞赛，曹老师最喜欢有事没事的扯他过去的学生的故事，以及竞赛圈里的八卦。也正是那个时候，从曹老师的口中，我知道了世界上还有ACM这么个玩意。还记得某一天，曹老师又在胡侃：“ACM比赛一共有4所大学夺得过2次世界冠军：上海交通大学，圣彼得堡ITMO，华沙大学还有斯坦福大学。”说出来不怕笑话，这也是我第一次听到斯坦福大学这个名字。所以斯坦福大学给我的第一印象就是：两次世界冠军的学校！说句题外话：因为恽之玮的原因，我喜欢了北大很长时间，也因为曹老师的这句胡侃，让斯坦福成了我最向往的大学；不过造化弄人，我的本科在北大的隔壁度过，而博士，又跑到了斯坦福的隔壁。</p>

<p>曹老师八卦的性格，也潜移默化的影响了我。平时没事的时候，就特别喜欢翻阅往年的OI比赛的成绩单，还有ACM世界总决赛的名单。直到现在，我都可以如数家珍的说出之前近10年的信息学竞赛中国国家队（甚至美国国家队）的队员名单，或者ACM世界总决赛清华，交大的队员名字，以及他们的名次。对于一个初中的孩子，翻着这些名字，听着这些故事，我无法用言语描述那是我心中的激动，好像这些故事，发生在一个无与伦比的，广阔的世界，而这些名字，就犹如夜空中的星星一样，闪耀，夺目。后来的很多年中，我很有幸几乎全部见过或者</p>

<p>接触过了这些当年在我心中犹如浩瀚星辰般的神们。他们应该很难想象，当我第一次见到他们的激动的心情。那种感觉，和歌迷见到他们的偶像一样。那时候心中埋藏很多年的小小的执念，也不过就是见他们一次。甚至直到现在，对一些人，还是这样。</p>

<p>初中的时候最喜欢参加省里的夏令营，冬令营，那时候年纪小，最喜欢跟在师兄的后面，听着师兄们讲着我听不懂的算法，打着我没玩过的游戏（我对仙剑的执念也是从那个时候开始的）；初三的暑假我有幸到了上海交通大学ACM队训练了一个月，作为一个初中生，我第一次见到了，国内顶尖的大学，顶尖的大学生是怎么样的，也遇到了我无比尊敬和感恩的，上海交通大学ACM班的俞勇老师；到了高中开始以非正式队伍的身份参加ACM区域赛，也就总算可以亲眼看一看，那些传说中的人，还可以冲上去和他们合一张影，然后激动的晚上睡不着觉；上了大学，大二暑假我到美国Facebook公司总部实习，在湾区见到了更老一辈的，那些如雷贯耳的名字。</p>

<p>光阴荏苒，时光匆匆流逝，好些画面在心里依然是那么清晰。讲讲那些名字吧。初中每次参加省里组织的信息学冬令营夏令营都会嚷嚷，我要是能见一面朱泽园（IOI金牌，ACM总决赛亚军，MIT博士）就好了，后来高二时在ACM杭州赛区总算见到了，当时还见到了他的两位队友：楼天成（楼教主，圈内人士应该都很熟悉）和周源（IOI满分，ACM总决赛亚军，CMU博士），当时捧着那张合影，简直比拿了冠军还激动；初三暑假在交大，第一次在机房里见到了仰慕已久的戴文渊（ACM世界冠军，目前就职于华为诺亚方舟实验室），后来每次只要有机会都要蹦蹦跳跳的跑去找戴文渊合影；高一在ACM南京赛区，第一次见到了唐文斌（ACM总决赛世界第六，Face++ CEO），以及他的两个队友王栋（IOI金牌）和龙凡（IOI金牌，MIT博士），当然还见到了因为比砸了心情不好而拒绝我合影请求的郭华阳（IOI金牌，就职于Facebook）。当时我可能做梦也想不到，过了若干年，我会和唐文斌一起坐在火车的餐车地上打牌，会和郭华阳在纽约曼哈顿睡上下铺。后来进了大学，和贝小辉成了好朋友，楼天成成了我的助教，胡伟栋（IOI金牌，ACM世界亚军）会请我吃饭；我更不会想到，再后来，张一飞（IOI两枚金牌得主，就职于Facebook）会给我推荐简历，我会和符文杰（中国唯一IOI，IMO双国家队成员，就职于Facebook）一起吃饭，和侯启明（NOI历史上唯一的满分，IOI金牌，浙江大学副教授）一起扯淡。当然了，进入大学以后，我不会再端着相机四处合影了。</p>

<p>现在看，这些名字也很普通，也很平常，很多过去的故事，可能在茶余饭后都不会被提起了。但是世界上毕竟有这么一个小故事曾发生过：在很多很多年前，曾经有一个小正太，每天念叨着这些名字，在心里埋下小小的梦想，然后一步一步的，他见到了这些他成天念叨的人，和他们成了朋友。多年以后，他的梦想本身，也许此时正化作一个淡然的微笑。</p>

<p>一个关于固执，坚持和自信的故事</p>

<p>从小学开始参加信息学比赛，高一开始参加ACM区域赛，一直到现在博士还在参赛。这么多年了，有时自己都会感慨。每年参加比赛这件事情，好像已经快成为我生活的一部分了。记得有一次在ACM区域赛的时候发言，我说ACM是我的初恋，现在看，可能ACM成了我的生活了吧。懵懂，热烈，坚持，淡然，其实爱情也不过如此，最后所有的一切，都化作了生活。</p>

<p>很多人问我，这么多年了你怎么还在参赛呢，花费这么多精力，时间，到底是为什么呢？其实我也快说不清楚了。很小很小的时候，那时候教数学奥赛的老师嫌我贪玩训斥我鄙视我的智商。小孩子自尊心总是特别强，想着我换个地方证明我比那些你喜欢的人都厉害，最后差不多是做到了；到了初中，我想比那些师兄师姐们都厉害，要做全国一流的选手，后来差不多也做到了；到了高中发现好像在我身边的师兄，同学超不过了（提一下，有一个高中师兄叫金斌，TCO世界冠军，ACM总决赛冠军亚军各一次，还有一个高中师兄叫吴卓杰，ACM总决赛冠军，目前都就职于Google），我就想我可以到了大学参加ACM，在ACM赛场上超过他们；到了大学，做工程的不断实习做项目，做科研的一篇接一篇的发论文，刷GPA的几乎每门课都是满分。我看看自己似乎没有一样能做到最好的，于是就想，要不然就做一个科研界ACM比的最好的，而ACM界又科研做的最好的人吧；再到后来，我知道世界上还有Percy Liang （斯坦福大学助理教授，ACM总决赛亚军）， Matei Zaharia（MIT助理教授，Spark的开发者，ACM总决赛第四）这些人，遂发现自己实在是差距太大了，实在连个理由都编不出了（也许可能是常州方言讲的最好的？）。可能这个时候，ACM对我而言，更像是对自己的敬礼。毕竟，自己已经坚持了很久很久，从江南，到帝都，再到美利坚加利福尼亚。就像唱一首歌，跳一支舞，写一段文字，哪怕根本没有观众，也总希望</p>

<p>能够最后落下伴着微笑的句点。做一件事不一定非要有一个意义，因为坚持的本身，就有意义，也因为，坚持总能伴着一份感动，时不时的在心中翻涌。</p>

<p>坚持是要有信念的，信念是什么？当然是“赢”。想想还挺不容易也挺可笑的，到现在还在乎着输赢，尤其输赢已经早就没有了意义。不过也不那么可笑，因为我并不执念。 “赢”也许只是对生活的热情的简写吧。小时候想打败所有的人，做最好的选手，结果过了十年参加国际比赛拿了块丢人的银牌，在中国队里垫了底；高中参加ACM，每次都为击败一支清华的队伍感到振奋，而当自己到了清华，第一次参赛就在清华内部排在了最后；大学幻想着以自己作为队伍的核心战斗力，代表清华拿冠军，参加总决赛，结果连续拿到了4次区域赛的亚军（清华的规矩是，只有冠军才有资格代表学校参加总决赛），并且其中三次都是在最后时刻被反超，同题数输罚时；大三那年总算进了总决赛，幻想着能够屌丝逆袭一次进个前八，结果最后惊险拿到了领奖队伍的最后一名（总决赛一共12支队伍可以获得奖牌，金银铜各4支，我在2013年的总决赛获得第12名），并且还输给了CMU（后来在CMU遇到了他们的教练，被调侃：“Oh, we beat you, right?”）；大四练了很久的俄罗斯风格的题目，想着再去一次在俄罗斯举办的总决赛，证明一下自己，结果在最后一次区域赛输给了当时完美发挥的交大，最后甚至连亚军都没有拿到。今年8月我在加州大学伯克利分校开始了自己博士生涯。很有幸，我找到了两个老朋友作为自己的队友，然后我们轻松的拿到了西北太平洋赛区的冠军。于是，我可以代表加州大学伯克利分校，参加5月份的ACM世界总决赛了。这是我的第二次总决赛，也将是最后一次。不过我希望后面的故事不要再继续按照之前的走势了。</p>

<p>这里着重说一下我大三那年（2012）参加ACM的经历。当年我们的队名叫Again And Again，理由很简单，队里面三个人，我，毛杰明（IOI金牌，目前在普林斯顿读博士），莫涛（NOI第一，目前在香港中文读博士），在过去的2年里（2010和2011）分别参加过8个不同的赛区，一共获得了其中的6个亚军，并且又在2012年的长春赛区，一起携手拿到了我们的第7个亚军。9次比赛7次亚军，我想这个记录也是挺难超越的了。</p>

<p>我是一个固执的人，一旦认定的事情很难再被改变；我也是一个幸运的人，大部分我认定的事情都起码没有失败。也许，老天真的会偏爱一个愿意一直仅仅因为喜欢而坚持的人吧。中学的时候，大部分时间都是我一个人呆在机房里，或者一个人在家里抱着电脑想问题，那时候参加编程比赛是有理有据的，因为可以保送大学，可以给学校争光，家长和学校也都支持。到了清华，事情就不一样了。我知道，现在参加程序比赛越来越多的有了功利性，拿奖可以保研，可以找工作，或者给学校给领导争光。不过这些理由在清华，都是不成立的。我的大部分天才同学们，都在北美最好的学校读博士，至于给学校和领导争光，我想也许我们拿到了世界冠军，可能会有机会上一次清华首页吧，不过这也只是我的猜测——毕竟清华从来没有在ACM总决赛上夺冠过。清华没有什么ACM队，没有ACM训练或者选拔，也没有什么奖金或者评优加分。在清华，ACM不过是众多学生活动中普通的一项。我们这些老人有一个老笑话：“如何在开幕式迅速识别出清华的学生？在那里做作业的！”在很多学校，也许有很多ACM队的黑话，传统，故事；而清华也有他自己的故事，这些故事可能最特别也最普通。特别在于，清华也有与众不同的故事，不过这些故事几乎没有单纯关于ACM的：比如鬲融的故事（清华的一段传说，计算机系历史最高GPA，IOI金牌，ACM总决赛亚军，普林斯顿大学博士），周源的故事，或者楼天成的故事；普通在于，这些故事并没有一个明确的标签，ACM比赛只是这些故事里面，很小很小的一部分。大家是清华人，要做最好的学生，要发论文，要搞好课业，要跑3000米，只是大家都喜欢ACM，觉得她有趣。</p>

<p>“有趣”二字，说来轻巧，可是，我似乎也找不出更好的词汇来评价ACM这项活动了。读博士了之后很多人劝我说：“你都读博士了还玩这些年轻人的活动。”我同意这个观点。不过因为我还是觉得参加ACM很有趣，所以我又固执的接着参加了。于是作为一个老博士，我又得继续忽悠着同为博士的队友们在周末抽出时间来训练，继续在闲着无聊的时候作着我自己关于“赢”的春秋大梦。</p>

<p>一个关于青春，热情和友谊的故事</p>

<p>参加ACM ICPC竞赛，给了我太多太多美好的回忆，让我收获了太多太多宝贵的友谊。每当回忆翻涌，总会感慨当时的纯粹，热情和真挚——自己也算，有一段奋斗过的青春。</p>

<p>初三的暑假和两个师兄，吴沛凡（目前在纽约大学读经济博士）和金斌在上海交大ACM队训练了一个月。期间每天上午做一套真题，下午修订和讨论。还记得那时候金斌第一次洗冷水澡的时候爆发出的惊人惨叫；记得最</p>

<p>后一部哈利波特问世，我晚上拽着沛凡给我恶补哈利波特；记得第一次知道交大的BBS叫未名，清华的BBS叫水木；记得第一次知道了当时还得审核大学生身份的人人网，当时吴卓杰还用人人网的bug玩上面买车位的网页游戏；记得第一次去问交大师姐一道题怎么做的时候红着脸踌躇了好半天；记得当时更老的师兄们骑车载着我们去吃盖浇饭；更记得第一次三人组队赢了交大一队时候的兴奋和激动；也记得第一次见到交大ACM班的俞勇老师时候的胆怯和害羞……那段时光是我整个中学生涯中最好的时光，每天都很快乐，每天都有进步。我不得不承认，从那时候开始，交大成了我高中时期最向往的地方：为了去交大而不去清华，我还在高三的时候和我的妈妈进行了接近三个月的激烈斗争。好多好多年过去了，当年的ACM队师兄师姐们，早就工作，或者快博士毕业了；三个坐在后座的小屁孩，现在两个在读博士，一个拿了世界冠军，去了谷歌；俞勇老师老了很多岁。据我很多交大的同学说，俞勇老师很严厉。也许正因为我没真正做过俞老师的学生，所以起码在我这里，俞老师给我的印象是一个温和负责的导师，每次我参加比赛见到俞老师，都会特别亲切的打招呼，交谈。也许我应该对我去了清华感到庆幸吧，因为，从此交大在我心里就只留下了她最美好的样子。</p>

<p>大学在清华碰到了毛杰明和莫涛两个死党，一起吃外卖，一起互黑，一起旅游。那时候我自封后勤队长（其实我在每一个队伍里的角色都是这个），负责研究去什么赛区，每次协调大家什么时候训练，训练什么题，还负责代购火车票飞机票以及和教练沟通，并且乐此不疲。他俩也待我不薄，因为毛杰明，我在大学里少走了很多弯路，又因为莫涛，让我多了一个特别铁杆的基友。</p>

<p>2011年福州赛区，只要另一支清华的队伍不能在30分钟内通过最后一题，我和毛杰明就能晋级总决赛。只是很可惜，20分钟后，那支队伍前，升起了对应颜色的气球。我还清楚的记得，在回程的飞机上，心情郁闷的我和厦门航空的空乘吵了起来。毛杰明不停的安慰我不用跟人一般见识。我后来又去过不少次福建，不过也再没做过厦门航空的飞机。</p>

<p>2012年我和毛杰明还有莫涛组队。那年的长春赛区，比赛前我们三个吃牛排刮出了一张奖励2元钱的发票，要知道当时我们已经拿了6次亚军了。莫涛觉得2元钱太晦气，还刻意第二天早上给服务员送出1元的小费希望讨个吉利。结果赛场上，我们站在中山大学的身后，近距离目睹了他们在比赛结束后5分钟得到系统返回的一个YES，并以罚时优势将我们反超并夺冠的欣喜若狂。更狗血的是，在接下来的由中山大学负责命题的天津赛区，有一道题目和之前在长春赛区中山反超我们的那题一模一样。于是，在中大的好心保送下，我们总算拿到了冠军。</p>

<p>2013年圣彼得堡的总决赛，从来不喝咖啡的我在赛前喝了咖啡，不过似乎咖啡对于紧张没有什么好的疗效。虽然这么多年来我每逢ACM比赛都会紧张，心跳加速，血压升高，不过那一天似乎大家都紧张的特别严重：场上三个人对着两道题看了1个多小时居然都不会做，而一出赛场就都会了。回忆到当时看结果公布时自己的焦虑和不安，我觉得以后估计也不会有机会能够让自己的心律比那个时候更快一点了。记得比赛入场的时候我和我的一个俄罗斯朋友，当时莫斯科大学的选手Sergey握了一个手，然后那一年莫斯科大学和我们都拿了铜牌。毛杰明和莫涛都说，估计是我吸来了莫斯科大学的仙气，把人家拉成了铜牌，把我们从胸牌，提到了奖牌。这事我后来一直没好意思和Sergey说，希望他不会在心里骂我。</p>

<p>2013年下半年，毛杰明去了普林斯顿读博士，我和莫涛拉来了陈高远（Topcoder Target，USC硕士在读）。我们三个大四狗组了个队，唤作“老流氓”。作为流氓头子，我愣是自己联系，办理手续，然后忽悠另外两个老家伙们掏出了之前的比赛的奖金买了飞往日本的机票，使我们队成为了近10年来清华第一支参加海外ACM赛区的队伍（清华由于经费不够，加上一些历史原因，是不参加海外赛区的）。比赛比的很糟，不过三个老家伙在富士山下泡温泉，在东京米其林餐厅吃寿司，爬东京塔也是挺值得怀念的。还记得当时莫涛痴迷某日剧（具体原因我后来黑了他好久），我们被忽悠着整个一天东京的行程都按照剧里男主和女主的约会路线进行。傍晚，在东京的台场，莫涛望着远处的彩虹桥，掏出了他的iTouch和我说：“你看，剧里就是这个位置，这个角度，这个时间！”那荡漾的眼神，一定会成为未来很多很多年，一个关于莫涛的经典槽点。</p>

<p>2014年，我来到了加州伯克利大学读博士。很幸运，我遇到了两个也再读博士的中国队友，姜碧野（NOI2009金牌，在清华ACM区域赛和莫涛一起拿过3次亚军，代表伯克利参加了2014年ACM总决赛）和赖陆航（数学博士，IOI2010金牌，代表北大参加了2011年ACM总决赛），组成了伯克利中国队。于是我又开始了我后勤部长兼队内主码的身份——反正我对这个角色早已驾轻就熟。我们很轻松的晋级了2015年5月份将在摩洛哥举行的ACM总决赛。这对我们三个人来说，都是第二次，也是最后一次总决赛，也将是竞赛生涯的最后一场比赛了。想想时间过得真快，从一个小孩到一个博士，关于青春的故事，也终要画上句号了。</p>

<p>这段青春里，有太多有趣的事，有趣的人。很多很多年以后，这些说不完、道不尽，也都化为老友相聚时的调侃和相逢一笑。我觉得，这也许就是生活里，最美的喜剧。</p>

<p>最后的一些话</p>

<p>这是我的故事，感谢我有这个机会能把我的故事写下来，也感谢前来阅读的人。ACM不是生活的全部，甚至连一小部分都算不上——就像，写完这些文字，我还得抓紧时间继续准备我的论文，继续我的研究工作。也确实是这样，一个人的生活应该是丰富的，充实的和多彩的。生活应当像浩瀚的宇宙，壮阔却又静谧，ACM不过是那绚烂星辰中的一颗。宇宙不应因为少了一颗星而暗淡，但是却可能因为多了一颗星而多了一段璀璨的文明。</p>

<p>「把这篇文章放在这里呢，是真的喜欢这种人生，这种经历。偶然看到，有是一种莫名的感动。。」</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[我这一生]]></title>
    <link href="http://atlasbl.cn/coding_for_interview/15391740524915.html"/>
    <updated>2018-10-10T20:20:52+08:00</updated>
    <id>http://atlasbl.cn/coding_for_interview/15391740524915.html</id>
    <content type="html"><![CDATA[
<p>2018-10-10/by kala</p>

<p>到底要经历怎样的生活才能回头对自己说我这一生没白来过。</p>

<p>你有没有曾问过自己，想要什么样的人生？我曾经很多次的问自己，想要的是什么，是寻找存在的意义还是寻找生活的乐趣，后来我发现，我想要的不过是活着，简单的活着。没有那么多烦恼，没有那么多的忧虑的活着。有的人除了活着还有梦想，还有向往，而有的人好像什么也没有。</p>

<p>无知而无畏，无欲则无求，而我想说的是无求则无欲，没有追求，没有向往，便没有了动力。一定要找到自己喜欢并想去做的东西，努力的做好它。这也许就是意义吧，时隔多年终于可以对自己说我做成过一件事。</p>

<p>最近有看过《一千零一夜》这个纪录片的一部分，里面有一节讲的是《生命不能承受之轻》里面有这样一段话深深的触动了我「他吻她动人的唇，那时她患了感冒，却使得她的生意更沙哑，比平常更迷人，而盖茨无比抗拒的意识到『财富能囚住并保存青春和奥秘，还有只要拥有许多华服便能永葆清新亮丽』，他也深深意识到黛西的存在『她像银子般闪耀高居在无虞而得意的生活中与底下艰难搏斗的贫寒人家处于两个世界』」。从农村到城镇，从城镇到城市，从城市到一线城市，看过的经历过的深深让我意识到差距真的存在，不仅仅是地位与金钱，更多的是眼界与格局。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[文章结构]]></title>
    <link href="http://atlasbl.cn/coding_for_interview/15392326678534.html"/>
    <updated>2018-10-11T12:37:47+08:00</updated>
    <id>http://atlasbl.cn/coding_for_interview/15392326678534.html</id>
    <content type="html"><![CDATA[
<ul>
<li>数据结构介绍，相关实现，应用</li>
<li>数据结构的操作图示</li>
<li>数据结构的相关实现</li>
<li>树的相关操作</li>
<li>数据结构的相关解题思路</li>
<li>数据结构的相关题目：链接Leetcode的题目<br/>
# 示例</li>
</ul>

<h2 id="toc_0">树</h2>

<p>树是一种抽象数据类型（ADT）或是实现这种抽象数据类型的数据结构，用来模拟具有树状结构性质的数据集合。它是由n（n&gt;0）个有限节点组成一个具有层次关系的集合。把它叫做“树”是因为它看起来像一棵倒挂的树，也就是说它是根朝上，而叶朝下的。它具有以下的特点：</p>

<ul>
<li>每个节点有零个或多个子节点；</li>
<li>没有父节点的节点称为根节点；</li>
<li>每一个非根节点有且只有一个父节点；</li>
<li>除了根节点外，每个子节点可以分为多个不相交的子树；</li>
</ul>

<h2 id="toc_1">树的实现方式</h2>

<ul>
<li>二维数组：每个数组是树的一个节点node,每个node存储了当前节点的数据和它的父节点索引。</li>
<li>字典实现：</li>
</ul>

<h2 id="toc_2">树的应用</h2>

<ul>
<li>二叉树
<ul>
<li>AVL：平衡二叉树，当且仅当任何节点的两棵子树的高度差不大于1的二叉树；</li>
<li>BST：排序二叉树，二叉查找树</li>
</ul></li>
<li>Huffman树：带权路径最短的二叉树称为哈夫曼树或最优二叉树</li>
<li>B树：一种对读写操作进行优化的自平衡的二叉查找树，能够保持数据有序，拥有多于两个子树。</li>
<li>B+树：B树的优化</li>
<li>RBT红黑树：</li>
<li>堆Heap：二叉堆，斐波那契堆</li>
<li>Trie树：</li>
</ul>

<h2 id="toc_3">二叉树的python实现</h2>

<pre class="line-numbers"><code class="language-text">def 
</code></pre>

<h2 id="toc_4">二叉树的各种操作</h2>

<ul>
<li>层次遍历</li>
<li>之字形遍历</li>
<li>先序遍历</li>
<li>中序遍历</li>
<li>后序遍历</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[堆：Heap]]></title>
    <link href="http://atlasbl.cn/coding_for_interview/15392363328863.html"/>
    <updated>2018-10-11T13:38:52+08:00</updated>
    <id>http://atlasbl.cn/coding_for_interview/15392363328863.html</id>
    <content type="html"><![CDATA[
<p>堆（英语：Heap）是计算机科学中的一种特别的树状数据结构。若是满足以下特性，即可称为堆：“给定堆中任意节点 P 和 C，若 P 是 C 的母节点，那么 P 的值会小于等于（或大于等于） C 的值”。若母节点的值恒小于等于子节点的值，此堆称为最小堆（英语：min heap）；反之，若母节点的值恒大于等于子节点的值，此堆称为最大堆（英语：max heap）。在堆中最顶端的那一个节点，称作根节点（英语：root node），根节点本身没有母节点（英语：parent node）。</p>

<p>堆始于 W. J. Williams 在 1964 年发表的堆排序（英语：heap sort），当时他提出了二叉堆树作为此算法的数据结构。堆在戴克斯特拉算法（英语：Dijkstra&#39;s algorithm）中亦为重要的关键。</p>

<p>在队列中，调度程序反复提取队列中第一个作业并运行，因为实际情况中某些时间较短的任务将等待很长时间才能结束，或者某些不短小，但具有重要性的作业，同样应当具有优先权。堆即为解决此类问题设计的一种数据结构「『优先级队列』」。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[图：Graph]]></title>
    <link href="http://atlasbl.cn/coding_for_interview/15392292553953.html"/>
    <updated>2018-10-11T11:40:55+08:00</updated>
    <id>http://atlasbl.cn/coding_for_interview/15392292553953.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[哈希表：Hash]]></title>
    <link href="http://atlasbl.cn/coding_for_interview/15392292427453.html"/>
    <updated>2018-10-11T11:40:42+08:00</updated>
    <id>http://atlasbl.cn/coding_for_interview/15392292427453.html</id>
    <content type="html"><![CDATA[
<p>。。。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[树：Tree]]></title>
    <link href="http://atlasbl.cn/coding_for_interview/15392291769523.html"/>
    <updated>2018-10-11T11:39:36+08:00</updated>
    <id>http://atlasbl.cn/coding_for_interview/15392291769523.html</id>
    <content type="html"><![CDATA[
<p>树是一种抽象数据类型（ADT）或是实现这种抽象数据类型的数据结构，用来模拟具有树状结构性质的数据集合。它是由n（n&gt;0）个有限节点组成一个具有层次关系的集合。把它叫做“树”是因为它看起来像一棵倒挂的树，也就是说它是根朝上，而叶朝下的。它具有以下的特点：</p>

<ul>
<li>每个节点有零个或多个子节点；</li>
<li>没有父节点的节点称为根节点；</li>
<li>每一个非根节点有且只有一个父节点；</li>
<li>除了根节点外，每个子节点可以分为多个不相交的子树；</li>
</ul>

<h1 id="toc_0">树的实现方式</h1>

<ul>
<li>二维数组：每个数组是树的一个节点node,每个node存储了当前节点的数据和它的父节点索引。</li>
<li>字典实现：</li>
</ul>

<h1 id="toc_1">树的应用</h1>

<ul>
<li>二叉树
<ul>
<li>AVL：平衡二叉树，当且仅当任何节点的两棵子树的高度差不大于1的二叉树；</li>
<li>BST：排序二叉树，二叉查找树</li>
</ul></li>
<li>Huffman树：带权路径最短的二叉树称为哈夫曼树或最优二叉树</li>
<li>B树：一种对读写操作进行优化的自平衡的二叉查找树，能够保持数据有序，拥有多于两个子树。</li>
<li>B+树：B树的优化</li>
<li>RBT红黑树：</li>
<li>堆Heap：二叉堆，斐波那契堆</li>
<li>Trie树：</li>
</ul>

<h1 id="toc_2">二叉树的python实现</h1>

<pre class="line-numbers"><code class="language-text">def 
</code></pre>

<h1 id="toc_3">二叉树的基本操作</h1>

<ul>
<li>层次遍历</li>
<li>之字形遍历</li>
<li>先序遍历</li>
<li>中序遍历</li>
<li>后序遍历</li>
</ul>

<h1 id="toc_4">二叉树的相关题目</h1>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[栈：Stack]]></title>
    <link href="http://atlasbl.cn/coding_for_interview/15392291360952.html"/>
    <updated>2018-10-11T11:38:56+08:00</updated>
    <id>http://atlasbl.cn/coding_for_interview/15392291360952.html</id>
    <content type="html"><![CDATA[
<p>栈（英语：stack）又称为栈或堆叠，是计算机科学中一种特殊的串列形式的抽象数据类型，其特殊之处在于只能允许在链表或数组的一端（称为堆栈顶端指针，英语：top）进行加入数据（英语：push）和输出数据（英语：pop）的运算。另外栈也可以用一维数组或链表的形式来完成。堆栈的另外一个相对的操作方式称为队列。</p>

<p>由于堆栈数据结构只允许在一端进行操作，因而按照后进先出（LIFO, Last In First Out）的原理运作</p>

<h1 id="toc_0">栈的操作</h1>

<p>堆栈数据结构使用两种基本操作：推入（压栈，push）和弹出（弹栈，pop）：</p>

<ul>
<li>推入：将数据放入堆栈的顶端（数组形式或串列形式），堆栈顶端top指针加一。</li>
<li>弹出：将顶端数据数据输出（回传），堆栈顶端数据减一。</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[队列：Queue]]></title>
    <link href="http://atlasbl.cn/coding_for_interview/15391808453198.html"/>
    <updated>2018-10-10T22:14:05+08:00</updated>
    <id>http://atlasbl.cn/coding_for_interview/15391808453198.html</id>
    <content type="html"><![CDATA[
<p>队列是先进先出（FIFO, First-In-First-Out）的线性表。在具体应用中通常用链表或者数组来实现。队列只允许在后端（称为rear）进行插入操作，在前端（称为front）进行删除操作。</p>

<p>队列的操作方式和堆栈类似，唯一的区别在于队列只允许新数据在后端进行添加。</p>

<h1 id="toc_0">队列的特点</h1>

<ul>
<li>先入先出</li>
</ul>

<h1 id="toc_1">队列的应用</h1>

<p>对于需要顺序执行的任务尝试用队列这种结构实现。</p>

<h1 id="toc_2">队列的简单实现</h1>

<h4 id="toc_3">1. Python List实现固定长度的队列,同样的方式可以实现双端队列</h4>

<pre class="line-numbers"><code class="language-python">class Queue(object):
    def __init__(self, init_size):
        self._data = [None] * init_size
        self._head = -1
        self._tail = -1
        self._size = init_size

    def add(self, item):
        if self._tail &gt;= self._size-1:
            print(&quot;queue full&quot;)
        else:
            self._tail += 1
            self._data[self._tail] = item

    def poll(self):
        if self._head &gt;= self._tail:
            print(&quot;queue empty&quot;)
        else:
            self._head += 1
            top = self._data[self._head]
            self._data[self._head] = None
            return top

    def foreach(self):
        rs = []
        while self._head &lt;= self._tail-1:
            top = self.poll()
            rs.append(top)
        return rs
</code></pre>

<h4 id="toc_4">2. 两个栈实现队列</h4>

<pre class="line-numbers"><code class="language-text">
</code></pre>

<h1 id="toc_5">队列相关题目</h1>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[链表：LinkedList]]></title>
    <link href="http://atlasbl.cn/coding_for_interview/15391807987921.html"/>
    <updated>2018-10-10T22:13:18+08:00</updated>
    <id>http://atlasbl.cn/coding_for_interview/15391807987921.html</id>
    <content type="html"><![CDATA[
<p>链表（Linked list）是一种常见的基础数据结构，是一种线性表，但是并不会按线性的顺序存储数据，而是在每一个节点里存到下一个节点的指针(Pointer)。由于不必须按顺序存储，链表在插入的时候可以达到O(1)的复杂度，比另一种线性表顺序表快得多，但是查找一个节点或者访问特定编号的节点则需要O(n)的时间，而顺序表相应的时间复杂度分别是O(logn)和O(1)。</p>

<p>使用链表结构可以克服数组链表需要预先知道数据大小的缺点，链表结构可以充分利用计算机内存空间，实现灵活的内存动态管理。但是链表失去了数组随机读取的优点，同时链表由于增加了结点的指针域，空间开销比较大。</p>

<h1 id="toc_0">链表的特点</h1>

<ul>
<li>不使用连续的内存</li>
<li>只能顺序访问，不能随机读取</li>
<li>插入，删除简单，查找慢</li>
<li>动态扩展</li>
</ul>

<h1 id="toc_1">常见链表</h1>

<ul>
<li>单向链表</li>
<li>双向链表</li>
<li>循环链表</li>
</ul>

<h1 id="toc_2">链表的实现</h1>

<p>Python List实现链表：</p>

<pre class="line-numbers"><code class="language-python">class Node(object):
    &quot;&quot;&quot;定义链表的节点&quot;&quot;&quot;
    def __init__(self, data):
        self._data = data
        self._next = None

    def get_data(self):
        return self._data

    def get_next(self):
        return self._next

    def set_data(self, data):
        self._data = data

    def set_next(self, next):
        self._next = next

class LinkedList(object):
    def __init__(self):
        self._head = Node(None)
        self._tail = Node(None)
        self._head.set_next(self._tail)
        self._size = 0
        
    def add(self, data):
        &quot;&quot;&quot;头插法：新加入的元素永远在head._next&quot;&quot;&quot;
        node = Node(data)
        node.set_next(self._head.get_next())
        self._head.set_next(node)
        self._size += 1
        
    def remove(self, data):
        &quot;&quot;&quot;删除节点&quot;&quot;&quot;
        prev = self._head
        while prev.get_next() is not self._tail:
            cur = prev.get_next()
            if cur.get_data() == data:
                prev.set_next(cur.get_next())
        self._size -=1
        
    def size(self):
        return self._size
        
    def is_exist(self, data):
        &quot;&quot;&quot;查询某值是否在链表&quot;&quot;&quot;
        if any([self._head.get_next() == self._tail, data == None]):
            return False
        cur = self._head.get_next()
        while cur is not self._tail:
            if cur.get_data == data:
                return True
            else:
                return False
                
    def is_empty(self):
        &quot;&quot;&quot;判断是否为空&quot;&quot;&quot;
        return self._head.get_next() == self._tail
        
    def foreach(self):
        if self._head.get_next() == self._tail:
            return None
        data = []
        cur = self._head.get_next()
        while cur is not self._tail:
            data.append(cur.get_data())
            cur = cur.get_next()
        return data
</code></pre>

<p>因为是自定义实现，使用了固定长度的List来实现，链表的重要操作难度是在删除和添加节点上，</p>

<p><strong>⚠️注意</strong>：<code>Head指针永远不变，是链表的开始</code>，不要直接使用head指针直接去进行指针移动，这样就无法返回操作后的链表了。</p>

<h1 id="toc_3">链表相关的题目</h1>

<p>全部来源于<a href="https://leetcode-cn.com/">Leetcode</a>中文网</p>

<h4 id="toc_4">简单:</h4>

<p><a href="">『删除链表中的节点』</a>,<a href="">『删除链表的倒数第N个节点』</a>,<a href="">『反转链表』</a>,<a href="">『合并两个有序链表』</a>,<a href="">『回文链表』</a>,<a href="">『环形链表』</a></p>

<h4 id="toc_5">中等</h4>

<p>``</p>

<h4 id="toc_6">困难</h4>

<p>``</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数组：Array]]></title>
    <link href="http://atlasbl.cn/coding_for_interview/15391804069666.html"/>
    <updated>2018-10-10T22:06:46+08:00</updated>
    <id>http://atlasbl.cn/coding_for_interview/15391804069666.html</id>
    <content type="html"><![CDATA[
<p>在计算器科学中，数组数据结构（英语：array data structure），简称数组（英语：Array），是由相同类型的元素（element）的集合所组成的数据结构，「分配一块连续的内存来存储」。利用元素的索引（index）可以计算出该元素对应的存储地址。</p>

<p>最简单的数据结构类型是一维数组。例如，索引为0到9的32位整数数组，可作为在存储器地址2000，2004，2008，...2036中，存储10个变量，因此索引为i的元素即在存储器中的2000+4×i地址。数组第一个元素的存储器地址称为第一地址或基础地址。</p>

<h2 id="toc_0">数组的特点</h2>

<ul>
<li>连续的内存</li>
<li>同一类型</li>
<li>相同大小</li>
<li>随机读取</li>
<li>插入删除耗时</li>
<li>初始化必须知道大小</li>
<li>扩容消耗时间，空间</li>
</ul>

<h2 id="toc_1">数组应用</h2>

<ul>
<li>实现List：List这种数据结构就是通过数组实现的</li>
<li>实现String：字符串这种数据结构就是通过字符数组实现的</li>
<li>实现：队列，堆，树，栈。</li>
</ul>

<h2 id="toc_2">数组相关的题目</h2>

<p><a href="15392386512557.html">两数之和</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[查找问题]]></title>
    <link href="http://atlasbl.cn/coding_for_interview/15393054098142.html"/>
    <updated>2018-10-12T08:50:09+08:00</updated>
    <id>http://atlasbl.cn/coding_for_interview/15393054098142.html</id>
    <content type="html"><![CDATA[
<p>最常见的查找就是<code>二分查找</code>，多数的查找问题的解决方案都是转化为选择合适的数据结构存储，再利用数据结构的特性来解决。比如：<code>Hash表</code>能够实现数据的直接定位，即O(1)的时间复杂度。<code>二叉排序树</code>，<code>红黑树</code>，<code>B树</code>，这些数据结构的设计就是为了解决数据搜索的问题。还有一些数据的查找如TopN借助于排序算法，</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[二分查找]]></title>
    <link href="http://atlasbl.cn/coding_for_interview/15395253854404.html"/>
    <updated>2018-10-14T21:56:25+08:00</updated>
    <id>http://atlasbl.cn/coding_for_interview/15395253854404.html</id>
    <content type="html"><![CDATA[
<p>二分查找是一种查询算法，它的前提是：<code>数据必须有序</code>，基本思想是不断的将数据2分(即不断的去缩小一半的搜索范围)最终定位找到查询数据。</p>

<h1 id="toc_0">二分搜索的步骤：</h1>

<ol>
<li>用数组的中间元素比较key,相等则返回中间元素的索引</li>
<li>如果中间元素大于key,则在左侧继续查找</li>
<li>如果中间元素小于key,则在右侧继续查找</li>
<li>直到定位到单个元素为止</li>
</ol>

<h1 id="toc_1">二分查找的实现</h1>

<h4 id="toc_2">1、递归实现</h4>

<pre class="line-numbers"><code class="language-python">def binary_search(nums, start, end, key):
    &quot;&quot;&quot;递归实现&quot;&quot;&quot;
    if start &gt; end:
        return -1
    mid = start + (start - end) / 2
    if nums[mid] &gt; key:
        return binary_search(nums, start, mid-1, key)
    elif nums[mid] &lt; key:
        return binary_search(nums, mid+1, end, key)
    else:
        return mid
</code></pre>

<h4 id="toc_3">2、非递归实现</h4>

<pre class="line-numbers"><code class="language-python">def binary_search(nums, start, end, key):
    &quot;&quot;&quot;非递归实现&quot;&quot;&quot;
    while start&lt;=end:
        mid = start+(start-end)/2
        if nums[mid]&gt;key:
            end = mid-1
        elif nums[mid]&lt;key:
            start = mid+1
        else:
            return mid
    return -1
</code></pre>

]]></content>
  </entry>
  
</feed>
